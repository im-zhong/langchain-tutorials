{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2025/6/24\n",
    "# zhangzhong\n",
    "# https://python.langchain.com/docs/tutorials/retrievers/\n",
    "\n",
    "from utils import load_env\n",
    "load_env()\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
    "\n",
    "# - page_content: a string representing the content;\n",
    "# - metadata: a dict containing arbitrary metadata;\n",
    "# - id: (optional) a string identifier for the document.\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
      "Albert Guâˆ—1\n",
      "and Tri Daoâˆ—2\n",
      "1\n",
      "Machine {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#loading-documents\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "file_path = \"mamba.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PyPDFLoader loads one Document object per PDF page\n",
    "# è¿˜çœŸæ˜¯ï¼Œæˆ‘ä»¬çš„manbaè®ºæ–‡æ€»å…±æœ‰36é¡µ\n",
    "docs: list[Document] = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].page_content[:100], docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#splitting\n",
    "# We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks\n",
    "# We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size\n",
    "# We set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute â€œstart_indexâ€.\n",
    "\n",
    "# blabla, just give me an example!\n",
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# è¿™ä¸ªç±»è¿˜å¯ä»¥è‡ªå·±æŒ‡å®š seperator ä¸è¿‡é»˜è®¤çš„seperatoråº”è¯¥å°±æ˜¯é€šç”¨çš„\n",
    "# å¯èƒ½åœ¨å¤„ç†è‹±æ–‡çš„æ—¶å€™æ¯”è¾ƒåˆé€‚å§ï¼Œå¤„ç†ä¸­æ–‡çš„å¯èƒ½éœ€è¦è‡ªå·±å†™ï¼Ÿæˆ–è€…ä»–æ˜¯æŒ‰ç…§æ®µè½ ç©ºæ ¼ æ¢è¡Œæ¥åšseperatorçš„ï¼Ÿ\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits: List[Document] = text_splitter.split_documents(documents=docs)\n",
    "len(all_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯ä»¥çœ‹åˆ°åœ¨metadataé‡Œé¢å¤šäº†ä¸€ä¸ª start_index ç”¨äºæ ‡æ³¨è¿™ä¸ªchunkåœ¨åŸæ–‡ä¸­çš„ä½ç½®\n",
    "# è¿™æ ·å°±å¯ä»¥åšå¼•ç”¨äº†\n",
    "print(all_splits[0].page_content[:100], all_splits[0].metadata)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "print(all_splits[0].page_content)\n",
    "print(\"----------------------------------\")\n",
    "print(all_splits[1].page_content)\n",
    "print(\"----------------------------------\")\n",
    "print(all_splits[2].page_content)\n",
    "print(\"----------------------------------\")\n",
    "# æˆ‘æ‡‚äº†ï¼Œçœ‹è¾“å‡ºæ•ˆæœï¼Œå°±æ˜¯æŠŠåŸæ¥çš„æ–‡æœ¬æŒ‰ç…§æŸäº›è§„åˆ™ï¼Œæ¯”å¦‚å›ºå®šçš„å­—ç¬¦æ•°ï¼Œæˆ–è€…æ®µè½ï¼Œç»™åˆ†å¼€ï¼Œç„¶åä¸¤ä¸ªç›¸é‚»çš„chunkä¼šoverlapä¸€æ®µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#embeddings\n",
    "# https://www.reddit.com/r/LocalLLaMA/comments/1iyun6z/using_deepseek_r1_for_rag_dos_and_donts/\n",
    "# æ‰¾åˆ°ä¸€ä¸ªå¥½ä¸œè¥¿ï¼Œè¯´äº†ç”¨åƒé—®çš„embeddingæ¨¡å‹å’Œdeepseekçš„reason æ¨¡å‹ å¯ä»¥\n",
    "# æˆ‘å»çœ‹çœ‹åƒé—®æœ‰æ²¡æœ‰APIå¯ä»¥ä¹°å§\n",
    "# https://github.com/Graph-RAG/GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "[-0.038538604974746704, 0.03440577909350395, 0.026333853602409363, -0.046597618609666824, -0.05496659129858017, -0.01645381562411785, -0.012773016467690468, 0.044376224279403687, -0.06524699926376343, 0.12873753905296326, 0.004181257914751768, 0.0032949603628367186, -0.011681691743433475, 0.012630950659513474, -0.006935399491339922, -0.012340361252427101, 0.0030915478710085154, -0.0640588104724884, 0.004010132979601622, 0.005624518264085054, -0.04623599350452423, 0.02319548837840557, -0.0238024964928627, 0.009350519627332687, -0.01554975938051939, 0.009763802401721478, -0.0293947272002697, -0.08591112494468689, 0.019953802227973938, -0.008388346061110497, -0.0488448403775692, 0.023789580911397934, 0.007342224474996328, 0.00339020905084908, -0.04357548803091049, 0.038254473358392715, 0.016208428889513016, 0.03921018913388252, -0.016867097467184067, 0.0042619770392775536, -0.036653004586696625, 0.005660034716129303, -0.06571193784475327, 0.025662269443273544, -0.012146634981036186, 0.006877281237393618, 0.009221368469297886, -0.06116583198308945, -0.03130615875124931, 0.064782053232193, -0.04827657714486122, -0.04132826253771782, -0.008698307909071445, -0.03745374083518982, -0.0096152788028121, 0.020341254770755768, 0.009854207746684551, -0.015343117527663708, -0.002461937488988042, 0.02000546269118786, 0.04763082414865494, 0.04915480315685272, -0.034328289330005646, 0.024047883227467537, 0.02585599571466446, -0.04478950425982475, -0.036988794803619385, 0.0060216570273041725, 0.014413231983780861, -0.020625386387109756, 0.009744429960846901, 0.057704586535692215, -0.053881723433732986, -0.01267615333199501, -0.06199239566922188, 0.054604969918727875, -0.04148324579000473, -0.015717655420303345, -0.04143158346414566, 0.009679853916168213, 0.0025781732983887196, -0.024990685284137726, -0.011565456166863441, -0.018416907638311386, 0.025920569896697998, 0.05075627565383911, -0.023440875113010406, -0.0774388313293457, 0.04683008790016174, -0.04117328301072121, -0.0065447180531919, 0.04559024050831795, 0.009473212994635105, -0.0037744329310953617, 0.018713954836130142, -0.006170180626213551, -0.0605459064245224, 0.025533117353916168, 0.013354195281863213, -0.030892876908183098, -0.032623499631881714, -0.0037744329310953617, -0.004952934104949236, -0.03600725159049034, 0.026256361976265907, 0.03528400510549545, -0.060287605971097946, -0.06385216861963272, -0.00922782626003027, 0.0302729532122612, 0.005127287935465574, -0.0018064971081912518, -0.0013916001189500093, 0.0066706398501992226, 0.013328365050256252, -0.017693663015961647, -0.029962990432977676, 0.00024034161469899118, -0.014271166175603867, -0.010299778543412685, 0.02056081034243107, 0.02568809874355793, -0.06571193784475327, -0.035723116248846054, 0.032571837306022644, -0.022730544209480286, 0.025752674788236618, -0.019127236679196358, 0.03515485301613808, 0.01591138169169426, 0.004817325621843338, -0.06064922735095024, 0.03463825210928917, -0.005692322738468647, 0.010932616889476776, -0.02121948078274727, 0.03487072139978409, 0.03319176286458969, 0.01722872070968151, 0.016105107963085175, -0.04236147180199623, 0.0006663375534117222, 0.06488537043333054, 0.01640215516090393, 0.002922037383541465, -0.00484638474881649, -0.01569182425737381, -0.042671430855989456, -0.004917417652904987, -0.02872314304113388, -0.03797034174203873, -0.013354195281863213, -0.01508481614291668, 0.009150335565209389, -0.004613913130015135, -0.054553307592868805, 0.03226187452673912, -0.042335640639066696, 0.015898466110229492, 0.027922408655285835, 0.02043166011571884, -0.03104785829782486, 0.03381168469786644, 0.009821919724345207, -0.03357921540737152, -0.019785907119512558, -0.07661227136850357, -0.02297593094408512, -0.017990710213780403, -0.013883713632822037, -0.012275786139070988, -0.011675234884023666, -0.020134612917900085, 0.051402028650045395, 0.038202814757823944, -0.028748974204063416, -0.010977820493280888, -0.004155427683144808, 0.028025729581713676, 0.05904775857925415, 0.01879144459962845, -0.00609268993139267, 0.14506220817565918, -0.01855897344648838, 0.06886322051286697, 0.0006029729265719652, 0.0033708366099745035, 0.041457414627075195, -0.02524898573756218, 0.03623972088098526, -0.024073714390397072, 0.033450063318014145, 0.032236047089099884, 0.017667831853032112, 0.023285893723368645, 0.027044182643294334, -0.016337579116225243, 0.07061967253684998, 0.0458485409617424, 0.023169657215476036, -0.012482427060604095, -0.037531230598688126, 0.0402175672352314, 0.006735215429216623, 0.05398504436016083, -2.4594151909695938e-05, -0.022498073056340218, -0.06395548582077026, 0.03174527361989021, 0.0023989765904843807, 0.0004447631654329598, -0.004888358525931835, -0.023389214649796486, -0.008381888270378113, 0.022330177947878838, 0.01469736360013485, -0.03967513144016266, 0.0006122556515038013, 0.013573751784861088, -0.02519732527434826, -0.01945011503994465, 0.04047586768865585, -0.0055793155916035175, -0.013844968751072884, -0.008388346061110497, 0.053055159747600555, -0.022691799327731133, 0.022665970027446747, 0.03164195269346237, 0.02823236957192421, -0.02413828857243061, 0.05011051893234253, -0.057652927935123444, -0.04636514559388161, -0.021335715427994728, -0.02319548837840557, -0.026153041049838066, -0.045099467039108276, 0.009511957876384258, 0.00980254728347063, 0.03735041990876198, 0.05142785981297493, -0.029575537890195847, -0.006192781962454319, 0.01904974691569805, -0.016260089352726936, -0.03812532499432564, -0.029808010905981064, 0.04269726201891899, -0.0168929286301136, -0.010177085176110268, -0.023841241374611855, -0.023298809304833412, 0.011894790455698967, -0.015046071261167526, -0.03104785829782486, 0.020186273381114006, 0.02524898573756218, 0.07382261008024216, -0.041457414627075195, -0.02602389082312584, 0.00016890506958588958, 0.028051558881998062, -0.002943024504929781, -0.011094056069850922, -0.00792986061424017, -0.03213272616267204, 0.0261788722127676, -0.06163077428936958, 0.01121674943715334, -0.019656755030155182, 0.025778504088521004, 0.010235202498733997, -0.027431635186076164, 0.0028397038113325834, -0.009260114282369614, -0.01651838980615139, 0.002744455123320222, -0.04608101397752762, 0.05039465054869652, -0.031848594546318054, 0.013741647824645042, -0.04104413092136383, 0.005088542588055134, -0.05408836528658867, 0.011533168144524097, 0.010719518177211285, -0.03623972088098526, -0.0367821529507637, 0.009699227288365364, 0.04827657714486122, -0.018688123673200607, 0.015343117527663708, 0.012385563924908638, 0.040579188615083694, 0.0008237400907091796, 0.0017806669929996133, 0.03497404232621193, 0.02944638766348362, -0.00884037371724844, -0.008827459067106247, -0.02408662810921669, -0.044608693569898605, 0.05414002388715744, 0.007865285500884056, 0.010261032730340958, -0.018261926248669624, -0.01707373932003975, 0.006128206383436918, -0.1265678107738495, -0.0017806669929996133, 0.05827285349369049, 0.024344930425286293, -0.03037627413868904, 0.03804783150553703, -0.007923402823507786, -0.043782129883766174, -0.026333853602409363, 0.03337257355451584, -8.717680611880496e-05, -0.029420558363199234, -0.03032461367547512, -0.002408662810921669, 0.012450139038264751, 0.043730467557907104, -0.02634676732122898, 0.0037550602573901415, -0.024551572278141975, -0.007316394243389368, -0.02187814936041832, -0.045822713524103165, -0.0010880957124754786, 0.020341254770755768, 0.06209571659564972, 0.03094453737139702, 0.02828403003513813, 0.01894642598927021, 0.057084664702415466, -0.05398504436016083, 0.0011938379611819983, 0.03657551482319832, -0.015872636809945107, -0.020082952454686165, 0.03866775706410408, -0.0013657700037583709, 0.024344930425286293, -0.0051046861335635185, 0.011475050821900368, -0.0023327867966145277, 0.008737053722143173, 0.006108833942562342, 0.029808010905981064, 0.0011244193883612752, 0.02353128045797348, 0.014516552910208702, -0.054449986666440964, 0.02993716113269329, 0.00015094502305146307, -0.028154879808425903, -0.005398504436016083, -0.007658643648028374, -0.015536843799054623, -0.043833788484334946, 0.03474157303571701, -0.028413182124495506, -0.02823236957192421, -0.04045003652572632, 0.009537788107991219, -0.017383700236678123, 0.01904974691569805, -0.028154879808425903, -0.05065295472741127, 0.00873059593141079, 0.020018378272652626, 0.02861982211470604, -0.007503662724047899, -0.02083202823996544, -7.602745608892292e-05, 0.029472218826413155, 0.004878672305494547, 0.010713061317801476, 0.010654943063855171, -0.04998137056827545, -0.024990685284137726, 0.021193649619817734, -0.005989369470626116, 0.012798846699297428, 0.08208826184272766, -0.006631894968450069, -0.030092142522335052, 0.02580433525145054, 0.005656806286424398, 0.0014650546945631504, 0.010144797153770924, -0.017745323479175568, 0.00679333321750164, 0.025765588507056236, 0.0033482350409030914, 0.0674167275428772, -0.041354093700647354, -0.005999055691063404, 0.009834835305809975, 0.015304372645914555, -0.003525817533954978, 0.006228298414498568, -0.024164119735360146, 0.021413207054138184, 0.007716761901974678, 0.004023048095405102, -0.04590020328760147, -0.04551275074481964, -0.033010952174663544, 0.05044631287455559, -0.004555795341730118, 0.013225044123828411, 0.0012164392974227667, -0.008536869660019875, -0.017370786517858505, -0.022717630490660667, -0.03081538714468479, 0.0125405453145504, -0.01433574128895998, 0.013128180988132954, -0.05899609625339508, -0.03693713620305061, 0.0712912529706955, -0.03585226833820343, -0.023608770221471786, -0.01717706024646759, 0.05204778164625168, -0.06555695831775665, -0.03618806228041649, 0.012630950659513474, 0.020741622895002365, 0.023079251870512962, 0.0009323075064457953, 0.04088915139436722, -0.03629138320684433, -0.07676725089550018, 0.013857883401215076, 0.03213272616267204, -0.0008935622754506767, -0.025597693398594856, -0.03275264799594879, -0.03735041990876198, 0.012766558676958084, 0.0007510927389375865, 0.020095868036150932, -0.0009121277253143489, 0.014038695022463799, -0.022407667711377144, -0.0017548368778079748, -0.019191812723875046, 0.01717706024646759, 0.013741647824645042, 0.012114346958696842, 0.04817325621843338, -0.034715741872787476, -0.036601342260837555, -0.004010132979601622, 0.0005634204717352986, 0.00836897362023592, -0.018094031140208244, -0.020948262885212898, -0.003068946534767747, 0.05137619748711586, -0.024603232741355896, 0.012023941613733768, -0.028697313740849495, -0.009963986463844776, 0.02905893512070179, 0.01549809891730547, 0.006392966024577618, -0.034999873489141464, -0.007535950746387243, 0.009240741841495037, 0.023156743496656418, 0.03406998887658119, -0.009679853916168213, 0.06700345128774643, 0.017551597207784653, 0.020534981042146683, -0.029420558363199234, 0.011332985013723373, -0.06726174801588058, -0.018429823219776154, -0.030531255528330803, 0.05625809729099274, 0.027044182643294334, 0.023453788831830025, 0.010680773295462132, 0.007445544935762882, 0.0038164069410413504, 0.014232421293854713, -0.03319176286458969, 0.05029132962226868, -0.013961204327642918, 0.01817152090370655, -0.03923602029681206, 0.012727813795208931, -0.00911804847419262, -0.020031291991472244, -0.019075576215982437, -0.042568109929561615, 0.012811761349439621, -0.01121674943715334, -0.0721694827079773, 0.003571020206436515, 0.035723116248846054, -0.04246479272842407, 0.011914163827896118, 0.04419541358947754, 0.045538581907749176, -0.04473784565925598, -0.011397560127079487, 0.0014166231267154217, 0.0162342581897974, 0.06963812559843063, -0.060080964118242264, -0.04336884617805481, -0.0062702726572752, -0.007871742360293865, -0.021865233778953552, 0.012295158579945564, 0.02403496764600277, -0.029756350442767143, -0.026889201253652573, -0.03856443613767624, 0.036317210644483566, 0.005572858266532421, -0.0005605953047052026, 0.0012067529605701566, -0.013405855745077133, 0.010177085176110268, 0.03055708482861519, 0.0596676804125309, 0.05132453888654709, 0.010906786657869816, -0.0375828891992569, 0.017370786517858505, 2.478333772160113e-05, 0.004655887372791767, -0.007710304111242294, -0.012682611122727394, 0.018804360181093216, -0.03396666795015335, 0.024280354380607605, -0.026863371953368187, -0.031409479677677155, -0.02602389082312584, 0.00664481008425355, -0.004704318940639496, 0.02337629906833172, -0.037815362215042114, 0.004746292717754841, -0.022510988637804985, 0.030195463448762894, 0.014025779440999031, -0.04045003652572632, 0.03363087400794029, -0.020909518003463745, -0.025868909433484077, 0.002292427234351635, -0.0428522452712059, -0.012805304490029812, 0.011533168144524097, -0.06225069612264633, -0.013470430858433247, -0.06276729702949524, 0.03406998887658119, -0.056878022849559784, 0.03528400510549545, -0.03280431032180786, 0.020625386387109756, -0.00011633663962129503, -0.006309017539024353, 0.011591286398470402, -0.005304870195686817, 0.017874473705887794, 0.00030289904680103064, -0.026656730100512505, -0.026863371953368187, -0.0024328785948455334, 0.022717630490660667, 0.021025754511356354, 0.03314010053873062, 0.007174328435212374, -0.010706603527069092, -0.01469736360013485, -0.004623599350452423, 0.023505449295043945, -0.023169657215476036, -0.033114273101091385, -0.0060216570273041725, -0.0034483270719647408, 0.04837989807128906, 0.03797034174203873, 0.01613093726336956, 0.004868986085057259, 0.032726820558309555, 0.040295056998729706, -0.03662717342376709, -0.025442712008953094, 0.012915082275867462, -0.017745323479175568, 0.010286862961947918, 0.017202889546751976, 0.028051558881998062, 0.012527629733085632, -0.025701014325022697, 0.1038372591137886, 0.022472243756055832, -0.010512877255678177, 0.004781809169799089, -0.014477807097136974, -0.004494448658078909, -0.020147528499364853, 0.005359759088605642, 0.014038695022463799, -0.02004420757293701, -0.024603232741355896, -0.011726894415915012, 0.014684448949992657, -0.027199164032936096, -0.005333928856998682, -0.021839404478669167, 0.007477832492440939, -0.0026911802124232054, -0.04866402968764305, 0.0008879119413904846, -0.07010306417942047, 0.0614241324365139, -0.018494397401809692, 0.022640138864517212, 0.04623599350452423, -0.019514689221978188, -0.05744628608226776, 0.005569629371166229, -0.015536843799054623, 0.013715817593038082, -0.005634204484522343, -0.0003727212024386972, -0.06679680943489075, -0.028903953731060028, 0.03171944245696068, 0.011171545833349228, 0.0003727212024386972, -0.017487021163105965, 0.014826514758169651, -0.0009613664587959647, 0.013677072711288929, -0.005708466283977032, -0.008582072332501411, -0.00033094899845309556, 0.07449419796466827, 0.017745323479175568, 0.018055284395813942, -0.06628020107746124, -0.011991653591394424, 0.009944613091647625, -0.05636141821742058, -0.03324342146515846, -0.024112459272146225, -0.007484290283173323, 0.034328289330005646, 0.03497404232621193, -0.009156793355941772, 0.000653826049529016, -0.01184958778321743, -0.004481533542275429, -0.0476049929857254, -0.003270744578912854, -0.023014677688479424, 0.025765588507056236, 0.07196284085512161, 0.045667730271816254, -0.02552020363509655, -0.011655861511826515, 0.05811787024140358, 0.012876337394118309, 0.0021148447412997484, 0.006722300313413143, 0.001772595103830099, 0.06364552676677704, -0.04055335745215416, 0.005740754306316376, 0.004449245985597372, -0.026062635704874992, 0.01002856157720089, 0.02568809874355793, 0.03900354728102684, -0.021232396364212036, -0.012753644026815891, 0.00544370710849762, -0.03683381527662277, 0.028929784893989563, -0.007645728997886181, -0.014658618718385696, 0.05321013927459717, 0.0484057292342186, -0.05199612304568291, 0.030040482059121132, -0.09340187907218933, -0.039365172386169434, 0.028490671887993813, -0.04522861912846565, 0.00445893220603466, -0.003942328970879316, -0.010809924453496933, -0.018261926248669624, 0.02960136905312538, -0.025326477363705635, -0.039365172386169434, -0.0002504315343685448, 0.0020018378272652626, 0.07563072443008423, 0.05889277532696724, 0.0244611669331789, 0.006825621239840984, 0.01955343410372734, 0.0016531305154785514, -0.04987804964184761, -0.0015199438203126192, -0.03252017870545387, -0.10182251036167145, -0.018442736938595772, 0.027974069118499756, -0.0036162231117486954, 0.026269277557730675, 0.020534981042146683, -0.03419913724064827, -0.06994808465242386, 0.04326552525162697, 0.002416734816506505, 0.017112484201788902, 0.021258225664496422, -0.021916894242167473, -0.0008854903280735016, 0.0027654420118778944, 0.00169833330437541, 0.007535950746387243, 0.0014763553626835346, 0.008678935468196869, -0.02479695901274681, 0.0925753116607666, 0.04517695680260658, -0.003593621775507927, 0.015020241029560566, -0.07878199964761734, -0.0527968555688858, 0.02612721174955368, -0.01894642598927021, 0.03724709898233414, -0.01088095735758543, -0.000986389466561377, -0.01657005026936531, 0.008052553981542587, 0.039184361696243286, 0.022291433066129684, -0.01220475323498249, 0.03138364851474762, 0.002029282273724675, -0.013909543864428997, 0.017319126054644585, -0.022472243756055832, 0.04861237108707428, 0.01044184435158968, -0.0188301894813776, -0.04812159761786461, 0.05661972239613533, -0.03347589448094368, -0.021129075437784195, -0.0035742491018027067, 0.03884856775403023, -0.023686261847615242, -0.013057148084044456, 0.023324638605117798, 0.00809129886329174, -0.010900329798460007, -0.019308047369122505, -0.004998136777430773, -0.02066413126885891, -0.06891488283872604, -0.006034572143107653, 0.026553409174084663, 0.009079302661120892, -0.02761244587600231, 0.008013809099793434, 0.006360678002238274, -0.0384611152112484, -0.018042370676994324, -0.02337629906833172, 0.036162231117486954, -0.0005117601249366999, -0.001175272511318326, -0.04633931443095207, 0.00930531695485115, 0.013302534818649292, 0.0614241324365139, -0.011617116630077362, 0.020625386387109756, -0.03629138320684433, 0.002746069338172674, -0.019398454576730728, 0.02663090080022812, 0.012663238681852818, -0.030479595065116882, 0.030195463448762894, -0.05584481731057167, -0.031022027134895325, -0.019153067842125893, -0.04680426046252251, 0.014296996407210827, -0.01668628677725792, 0.019398454576730728, -0.0006033764802850783, -0.0328301377594471, 0.01210143230855465, 0.00444601709023118, -0.0675717145204544, 0.006702927872538567, 0.004287807270884514, -0.046597618609666824, -0.014529467560350895, -0.012172465212643147, -0.017913218587636948, -0.02635968290269375, 0.013302534818649292, -0.0017467648722231388, 0.013806222938001156, -0.03569728881120682, 0.03585226833820343, -0.02745746448636055, 0.013237959705293179, -0.024538656696677208, 0.025507288053631783, -0.008375431410968304, -0.026863371953368187, 0.025546032935380936, 0.012404936365783215, -0.023105083033442497, -0.03081538714468479, 0.014555297791957855, 0.037531230598688126, -0.01762908697128296, -0.001168814953416586, 0.009266572073101997, -0.0013463973300531507, -0.011927078478038311, 0.003645282005891204, -0.08668603003025055, -0.002936566947028041, -0.0005153925158083439, 0.00455902423709631, 0.03076372668147087, 0.021568188443779945, 0.030453763902187347, 0.03662717342376709, 0.013186299242079258, -0.012785932049155235, -0.004371755290776491, -0.016983333975076675, 0.042335640639066696, 0.0462876558303833, -0.03972679376602173, 0.01557558961212635, 0.03032461367547512, -0.02872314304113388, -0.08260487020015717, 0.019269302487373352, -0.027354145422577858, 0.033062610775232315, 0.01251471508294344, 0.03148696944117546, 0.03812532499432564, 0.003696942236274481, 0.007464917842298746, 0.012682611122727394, 0.025391051545739174, 0.009712141938507557, -0.03174527361989021, 0.012004569172859192, 0.017047908157110214, -0.007503662724047899, 0.016544220969080925, -0.009679853916168213, -0.0009734743507578969, 0.01469736360013485, -0.020638301968574524, 0.0017742094350978732, -0.0073551395907998085, -0.014710279181599617, 0.02280803583562374, 0.0721694827079773, 0.012004569172859192, -0.05827285349369049, -0.03259766846895218, -0.019036831334233284, -0.008291482925415039, 0.004287807270884514, -0.022110620513558388, -0.028206540271639824, -0.007891115732491016, 0.0013754562241956592, 0.046158503741025925, -0.03130615875124931, -0.02061247080564499, -0.0009750887402333319, -0.024512825533747673, -0.048973992466926575, 0.023389214649796486, -0.02779325656592846, -0.006008741911500692, 0.00444601709023118, 0.010293320752680302, -0.02823236957192421, -0.025003599002957344, -0.01088741421699524, 0.006237984634935856, 0.006974144373089075, 0.012999030761420727, 0.037427909672260284, -0.017654918134212494, 0.026140127331018448, -0.030634574592113495, 0.031951915472745895, 0.010919702239334583, 0.03174527361989021, -0.018713954836130142, -0.030892876908183098, -0.05553485453128815, 0.05243523418903351, -0.018649378791451454, 0.006008741911500692, 0.05563817545771599, 0.03895188868045807, -0.013237959705293179, -0.04039837792515755, 0.010299778543412685, 0.001699947752058506, -0.02469363808631897, 0.0054049622267484665, 0.04125077277421951, -0.006857908796519041, 0.01988922618329525, -0.012023941613733768, 0.02464197762310505, 0.02496485412120819, -0.020070038735866547, -0.02663090080022812, -0.05527655407786369, -0.06467873603105545, 0.04484116658568382, -0.028439011424779892, 0.029472218826413155, -0.0253006462007761, -0.003454784629866481, -0.03468991070985794, -0.05512157082557678, -0.026475919410586357, 0.0410957932472229, -0.00719370087608695, -0.01833941787481308, -0.027224993333220482, 0.0021891065407544374, 0.017758239060640335, -0.018817275762557983, 0.034664079546928406, 0.010919702239334583, -0.04243896156549454, 0.01311526633799076, 0.0376603789627552, -0.001769366324879229, -0.03536149486899376, -0.07542408257722855, 0.0023182572331279516, 0.008336685597896576, -0.05661972239613533, -0.04187069833278656, -0.027276653796434402, 0.018029455095529556, 0.03381168469786644, -0.006028114818036556, 0.019966717809438705, -0.023130912333726883, -0.026333853602409363, 0.00021067728812340647, -0.016815437003970146, -0.028258200734853745, 0.01977299153804779, 0.01762908697128296, 0.011468593031167984, -0.012895709834992886, 0.04442788287997246, 0.05209944397211075, 0.026966692879796028, 0.006525345612317324, 0.017758239060640335, -0.010971362702548504, 0.000983160687610507, -0.035826437175273895, 0.010248118080198765, -0.03481906279921532, -0.021748999133706093, -0.04530610889196396, -0.011106970719993114, -0.079556904733181, 0.007949233055114746, -0.02668255940079689, 0.004726920276880264, 0.003015671856701374, 0.011772098019719124, -0.006121749058365822, -0.04352382943034172, 0.06767503172159195, 0.06385216861963272, -0.023905817419290543, 0.017861558124423027, -0.02049623616039753, -0.02597223035991192, -0.023828327655792236, 0.028800634667277336, -0.014619872905313969, -0.013341279700398445]\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/integrations/text_embedding/dashscope/\n",
    "# æ‰¾äº†åŠå¤©ï¼Œç»ˆäºæ‰¾åˆ°äº†è¿™ä¸ª\n",
    "# https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2842587.html\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v4\"\n",
    ")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(len(query_result)) # é»˜è®¤æ˜¯1024ç»´çš„å‘é‡\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "['b0121721-71a9-480e-a649-7249a6c1c6fa', '45f10c2b-6f32-4280-8bd5-b4e3154505f5', 'f177c9ea-6b26-4462-ba91-58e7c51a85ae', 'b1121ab5-87a1-46a1-888b-468fdbb2e990', 'be8e3550-7e03-4ace-9d3f-233fcc5e760b', 'cbac5b4e-782a-4861-bc24-7698e84aa450', '7faff6b0-04bd-48df-814b-6c631aecde18', 'defba535-b968-4c69-b5e0-45c0d4687df7', '909ed835-bfee-4429-8ddd-a5588f2f2a52', 'e89ef323-423e-423c-83e3-3a9287dca718', '6282ef9d-e33e-46ec-a32c-eb19cb517f7f', '8e26655f-6544-4000-a516-d37edc0977cb', '8bc666a1-76a2-4c9c-a76e-dc50dbb0e0f2', '0c7e423a-de6d-4f0c-92d5-d37bedfd73c4', 'f7f54e86-62dd-422e-8b55-5d5149ec242c', 'a39a658d-6608-4221-bfb4-caede0c70d29', '5c475ff8-5f38-4aab-a2a0-b522b8715d00', 'bc5618de-831b-4db8-8745-efe4dfb2d0fe', '922e6227-01d8-449a-8838-95047219825d', 'a3286e14-1354-42b3-a0ef-da3de069cbbc', 'bb6421b1-e18b-4541-9cb2-965803243e19', '74fbbbd3-909d-4605-9819-fc047d82debe', '7b7be6fe-8c63-44ad-bd4b-93fb2a6e4ca3', '855d54ae-70a8-49b5-a774-1c5f51349c3c', 'c279cbe9-cdc1-4f5b-a78b-49932cf8f1c3', '0ac0e9bc-fc8b-4592-8465-44366f3416fe', '26dcefca-d77d-475d-bfc1-9f6c3861bb03', '7469b255-e53b-4a94-a7ed-dcaa13518594', '105b0644-9950-4017-93b9-d21d8a287d74', 'a707a60f-3a3f-4d60-bd83-c7d1110c2293', 'f77a3d45-1942-4b7e-910e-7838f9a251ca', '99bf5cec-5082-49b4-a415-1f6f417e1c5c', 'cd9eead8-d014-4dd5-ba2f-0c8e5a58c245', '7fb38c17-fa7d-46a9-abef-23754630a124', 'fc5f2c4e-0af6-49e9-97a7-67e70904e971', 'f16fbfd3-7b5f-4a5d-bd8e-fd352a1e7b00', 'f32d4498-58d2-48d8-8e7e-3bcd7f4b29f0', '32e6f2e5-d080-43b4-a2f5-e3f65e672668', '53f9c0ea-e771-4543-8e06-80e1ea352565', 'c7bd32ff-2a11-4511-88fa-ede1a5855ea1', 'd235e606-8f56-468c-b918-524d7995db5a', 'ad4bb55a-72be-4635-9d6a-314ba9cfee4d', '08763aaf-fe4b-45ec-a536-bc265b11042e', 'e4e5472b-f75c-402f-97c7-27a515fc1aab', 'f72bd988-a512-45f5-9d61-aebb8748d40c', 'cbafe45a-4f8d-42d2-9ea0-ead5a4330027', 'a253ba1d-b046-4a44-aec1-fcafe1a0c872', '2cd12120-8534-4bee-9bca-1e26d1933c28', '6502e205-a7e4-4b3a-ae2e-87ff0e6dc620', '873c8d8e-8899-4890-b86d-706a32c9b295', '3fca998b-0907-49b9-987c-71257dce511b', '26b41ebd-e5dd-4e69-954c-450976a1e7a2', '6494d246-ff13-4c42-85f6-86fde353bfb8', '798da949-f39d-46bf-8fcc-4321e85d4ebb', '98116e8b-a600-46b1-bd8a-bea6168d2985', '08cb2c90-c85a-417b-bf63-15bc59881627', '24d7c986-d9ba-403e-8677-1acfc1c7985c', '6d458c4f-32aa-4cef-a6bd-f1eab2dd0bd9', 'f0ef0d14-33ef-4611-82e3-1d1db17a3e5d', '40be6a84-0d09-4b9f-9ede-300b655eabad', '5cdbd6b4-9de6-4e42-91f8-7dce3547e7f2', 'ea9c2372-3320-472f-85d0-2819186c28f6', 'fe73815d-1424-4ff3-bd6c-5db865c542ff', '44313cac-3371-483c-b4f3-9d6f64e1e0d3', 'b630ff48-f3d9-4b2e-b845-274156004e21', 'fb0a03f4-0a84-438e-8b6b-b130af66ef04', '8397b2f2-0324-42df-a645-baa38ff90fd9', '5ef7d50d-72d3-4fe9-9a05-9310f1e0de5e', '49e92d34-fdf4-4c0b-a8d6-7cb57c424c50', 'c6ae7b42-4bb9-4491-8301-ebb34eaf59af', '04d06bc6-eb0e-4bc2-b13e-1c6af4aa107d', '23a8bef7-d155-4a24-b7d4-0384b1a51fb6', '1ffb5075-5055-4852-91b0-e80e31485db1', '650cd7d3-f4bd-4334-ba9a-7e97f040e43b', '08650daa-aa85-4e8c-8025-1d21ac908b03', '4606ab40-7cdf-40be-8a20-b6dc323cbe47', '7aa931d0-6633-429d-bb0e-559c5e63a843', 'c2035bab-eb11-492e-b1a7-583154893fdf', '7704af5e-4739-484b-a4a8-0be453a9451a', '62b8d593-f326-4913-9005-4e1cded82219', '9d9d335a-7776-4ea4-b630-1463e492c0cb', 'a9fc76f1-d831-4b23-b621-2fb256b717dc', 'f8bad5b4-aebe-4fad-8a17-23e86d46af80', '54fa3ece-b1c1-4c73-a12d-e06ed19a83a6', '0976947e-a659-4ce8-9978-3bc7ee672eb1', '10016250-7ffc-4b98-8fa4-d1f249d80729', '42109253-d250-489b-a0e7-c27d867a28a2', 'f143b8aa-5b89-479b-851c-ebe80ba5cb36', 'ac710d3e-af2f-4c9e-a15f-47d739441525', '062db188-e413-4309-8287-13de02f475a7', 'dd9187af-6721-40ea-82ce-04a57aebe2ff', 'ad31112d-fcef-4543-b773-ffce4fea994d', 'fd4f428a-e60f-4547-bc22-262e3d8cee36', 'd61b3f22-7150-46e6-b088-116c44d77d1a', 'd79c87b9-f246-4ed0-bb71-84d1091fb822', 'a2cfe922-7972-461a-bcff-d1482916eb9a', '645f92a0-0207-4391-82cc-4b77f060b699', 'd42e3071-2f1a-4052-bab4-f6da49c17189', 'cac25cba-061a-4668-9051-d7cac5db707e', 'c38f7979-17f9-46aa-a412-3e094f7088c0', 'cd540264-7212-48b9-b82f-aa44ac52fe4f', '0769ce6b-77d6-49a1-8046-0ac18f3033c8', '933bea62-c916-4406-b9f7-b38830bdf4be', 'a12242e5-9d57-4ab1-92ff-fabf2146fa3d', '6d6b5fb4-3903-404f-ba8f-588d02f8e397', '9e21c6e0-1e2b-4190-b735-ab9c54a6e22e', '3ce701e1-9da3-43cd-b98b-2a30e3975077', '85eb8f3a-91ad-4106-8ae9-92545ef37a1b', '9cdbffc3-8b53-419c-972e-ff1e6a999c6a', 'd98bba7f-a873-41be-bc0e-c3ad49add594', '7b60486c-f1f4-4ecc-8249-6c6a1d3c64af', 'ad2fc77d-157a-4145-a563-60e5481a18dc', '4bb3fb37-6001-48b5-ae7a-ba91243ed998', '6d4f17ff-c10f-4f7f-9ee0-daef3b82e540', '52b625cc-199c-4d5d-8754-eb457c0769cd', '7d04311f-2fb9-4525-a74d-d8795a4b2290', '9053edc6-486b-412d-8e96-9eb1eb4b747b', '7e44b13d-f1d8-41ac-bac0-ccd659a74d58', '51608376-9f1c-4029-bb32-89325e283dd1', '8ee9bae4-378a-4440-873d-55b053a142bf', '69fa6b75-4728-4381-a2f5-6d1ec72ad0cb', '2ebc5a06-c324-4ebf-a451-e7f3eddb3818', '0fc6fdf4-4a1c-46c0-9a23-55c66a049123', 'e8496370-8023-4f0d-a667-ddb951021152', '34201dc1-1ba5-478d-ad80-9ae9d40403b7', '7312a51b-ff9e-464e-8f22-19b6093d4f81', '9fab4823-7816-41eb-a90e-537fb425b301', '74be3c36-3e70-4eda-b262-0b7b4068794e', '7a2dedf2-9935-475b-b0ce-c827d2d0a1ba', '75845b3f-16d6-4fbd-bd63-b202c6cb9f49', '3ed59bee-f1bb-483d-94d4-bb421ef8aeff', '8c1d1806-35e4-4cd5-b6cd-ae569fbf1332', '9e72ecd0-7bbd-4cc8-9733-dcf65d7d0eb5', '335d2709-feec-4ddc-9cac-c05671955ae1', '87a3e781-25d3-4beb-b680-d99ce544929e', 'ea405b24-08e2-44a3-b47e-5ab2e567b7dd', '3c8622de-893c-4db6-9174-d48dd667ab3d', 'c68fc0ca-bdaf-42b2-81ae-712485b4d041', '1a8bedd1-c5b7-4956-9cdd-d158d54f198b', '970df476-c387-4955-8177-bd247fe942fe', 'a32a6289-1f3c-47ca-8780-8839c03486d4', '0ecb57ff-7dfd-45fd-a32e-847cac4c2682', '687a566b-f0cb-4c24-92b6-2742e5b9d629', '9f5ff685-9d0a-4bc7-b3b2-1f13264cc67d', '3bb81c18-5db1-450a-a239-3e053aedb4a0', '7c786f4f-3c6c-4fc9-a464-dfba16432498', 'bc9906ae-0a7d-46e5-9cdf-0098018e3259', '7781bdf2-dd4b-4aaf-944d-5e9af79dac2a', 'f31e823d-fbb4-4534-83e3-0b49bd07fbc4', '5356fbe6-944c-46d6-b39d-72ca4e8dc56a', 'e4856133-bc0e-44ac-9e0b-4b834b15338c', '1eea59ef-fdef-44fc-8691-b7b6d4ae47f1', '84e18457-bdc2-4cb6-8ebc-d40424071300', '690e3461-9411-470e-9349-216b23c7102c', '08dbd936-ea17-4427-a22a-a6ff1c91ef15', 'c1d6cce0-2ba1-4f46-a2b3-21303ad79ab3', 'eb2204f8-0064-4a3a-83f4-c07182999ef6', 'c4e6ab74-7af8-457a-bf96-a30ab2a12981', '6ced28d2-af89-40d7-8b17-bf99535bcd12', '7d96fc15-c992-4981-bfed-c2fe622f09ae', '00c828e1-73e9-4e47-82fe-14d6c9d16eae', '7904baa3-1593-411c-9016-b8b4d06722b0', 'f3a19b63-6d7a-4a81-b206-9edffdfd0cf3', '8e7005c7-b5c3-4ee0-b855-ddaf71df11d9', 'e913fd2f-260a-499c-beca-8072c026e140', '3da1da55-682e-4a4c-85f4-1ec0c7b616c3', '0f19898f-9b37-43fe-836b-67f4140151e7', 'b42bc9ab-ce5d-4b92-b776-7d92c60a9db0', '1b7a8ac5-de7d-4cc6-8673-4d464c2ca763', 'a4bf5dd3-688b-4fd5-9c95-f8d3b01069b3', '4b66c2dd-c334-4c54-bb2d-7ee037e446b5', '988678e6-2a6d-490f-b5e6-0162ea5df00e', '8cc02c94-f0be-48b4-ac57-c03ce09be71a', '42f3f78c-ed07-4f11-8086-9810b6b0e833', 'e0422e54-bad7-4de8-a893-c83156fe240e', '5489e1bc-c55a-44d9-af58-56453e1f9f8f', '71135b50-e96b-4a09-b00f-d52a72be7fad', '9ef17096-9728-4235-9172-c9dea52f648e', '8c0b065f-23f9-4f8f-9a4a-dbd73207e289', 'd41d8119-db6f-48a2-9c36-583e5c2ad779', '629d5894-de16-4683-a8dd-17d9af028a0f', '46aa4968-951c-439b-8f9e-f89e6e7edd41', '02507d81-3778-4788-b1bd-517919baa42d', '201ba3fa-bc8d-4158-9ac1-44b2103f00d9', 'efea346a-094d-4d41-ac2d-680311c95d45', 'd8e2d00d-aecf-47b1-87fb-6e23ace58350', 'edd0970e-940c-44b9-ab28-1dfcceb48bb5', 'b10db90b-6335-4c34-87a2-9ef770484e71', '82105a4d-7dcf-4e37-8ddc-04fab09a4c5e', '333acb58-91b2-419d-9c17-556c5ba9e1d9', '49058353-c623-4860-80ff-25b88519b16a', 'f742e875-7296-42e3-a01f-0e9829c828dd', '0c03ef31-6076-4b6d-bc78-4fa84e3848bc', 'e92b4e2d-6569-48ca-a264-18e6b7faeab1', '86236105-048c-4664-85f2-3973b2d0e023', 'd56f41ea-78f0-4b32-b51a-165024f1523e', '9934d23d-9bea-4718-958e-929405b78814', '3cabf40b-90ba-4745-ba3f-991bf0e9e7ab', 'ec4fca7b-734c-4dfc-b350-bec8b205fd12', '031d5d0d-db58-4983-9b09-9bc972fe424b', 'c44538ac-2a26-4828-8f8d-7c6207a9637c', '48b1933d-8753-43b2-a30c-4043ae471a96', '4b07cac3-d725-413b-9bc5-07cb149ab65f', '527650cb-2d4c-4348-a74a-d4e196c27a88', '8b539f26-2601-42d9-b5c3-7abaf9164d2b', '0a1b4e9e-e41d-4247-b162-1b4f9934d9a4']\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#vector-stores\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# create vector store by passing in the embeddings model\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# add documents to the vector store\n",
    "# qwen embedding batch size should not large than 10\n",
    "# æ‰€ä»¥è¿™é‡Œéœ€è¦åˆ†æ‰¹å¤„ç†\n",
    "ids = []\n",
    "for i in range(0, len(all_splits), 10):\n",
    "    ids.extend(vector_store.add_documents(documents=all_splits[i:i+10]))\n",
    "\n",
    "\n",
    "print(len(ids))\n",
    "print(ids)\n",
    "# è¿™é‡Œidsçš„é•¿åº¦å’Œall splitsçš„é•¿åº¦æ˜¯ä¸€æ ·çš„\n",
    "# ä¹Ÿå°±æ˜¯æ¯ä¸ªdocumentéƒ½æ˜¯è¿”å›ä¸€ä¸ªid\n",
    "# é‚£ä¹ˆè¿™ä¸ªidæ˜¯å¹²å˜›çš„ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "modestly with a small increase in parameter count.\n",
      "Of particular note is the dramatic improvement of the selective SSM when the state size ğ‘ is increased, with over a 1.0\n",
      "perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1\n",
      "and 3.3.\n",
      "5 Discussion\n",
      "We discuss related work, limitations, and some future directions.\n",
      "Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an\n",
      "extended related work of SSMs and other related models.\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\"What is the main idea of the document?\")\n",
    "print(len(results))\n",
    "# è¿™ä¸å¯¹å§ï¼Œè¿™è¿”å›çš„ä¸œè¥¿å’Œæˆ‘ä»¬çš„é—®é¢˜å®Œå…¨æ²¡è”ç³»å•Šã€‚ã€‚ã€‚\n",
    "print(results[0].page_content)\n",
    "\n",
    "\n",
    "# è¿˜æœ‰asyncçš„\n",
    "# results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
    "\n",
    "# è¿˜å¯ä»¥é€šè¿‡embeddingè¿›è¡Œæœç´¢\n",
    "# å…¶å®æˆ‘ä»¬ç”¨æ–‡æœ¬è¿›è¡Œæœç´¢ æœ¬è´¨ä¸Šå†vector storeé‡Œé¢è¿˜æ˜¯ä¼šè°ƒç”¨embeddingæ¨¡å‹ ç„¶åå†åœ¨å‘é‡æ•°æ®åº“é‡Œé¢åšæœç´¢\n",
    "# embedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n",
    "# results = vector_store.similarity_search_by_vector(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='c38f7979-17f9-46aa-a412-3e094f7088c0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 15, 'page_label': '16', 'start_index': 2440}, page_content='modestly with a small increase in parameter count.\\nOf particular note is the dramatic improvement of the selective SSM when the state size ğ‘ is increased, with over a 1.0\\nperplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1\\nand 3.3.\\n5 Discussion\\nWe discuss related work, limitations, and some future directions.\\nRelated Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an\\nextended related work of SSMs and other related models.\\n16'),\n",
       "  Document(id='fe73815d-1424-4ff3-bd6c-5db865c542ff', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 10, 'page_label': '11', 'start_index': 7771}, page_content='recipe (Transformer++) that has now become standard, particularly as the sequence length grows. (We note\\nthat full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that\\ncan also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealistic\\ncomputation requirements.)\\n11'),\n",
       "  Document(id='08763aaf-fe4b-45ec-a536-bc265b11042e', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9', 'start_index': 827}, page_content='while a small Î” persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system\\ndiscretized by a timestep Î”, and in this context the intuition is that large Î” â†’âˆ represents the system focusing on the\\ncurrent input for longer (thus â€œselectingâ€ it and forgetting its current state) while a small Î” â†’0 represents a transient\\ninput that is ignored.\\nInterpretation of ğ‘¨. We remark that while the ğ‘¨ parameter could also be selective, it ultimately affects the model\\nonly through its interaction with Î” via ğ‘¨ = exp(Î”ğ‘¨)(the discretization (4)). Thus selectivity in Î” is enough to ensure\\nselectivity in (ğ‘¨,ğ‘©), and is the main source of improvement. We hypothesize that making ğ‘¨ selective in addition to (or\\ninstead of) Î” would have similar performance, and leave it out for simplicity.\\nInterpretation of ğ‘© and ğ‘ª. As discussed in Section 3.1, the most important property of selectivity is filtering out'),\n",
       "  Document(id='ad4bb55a-72be-4635-9d6a-314ba9cfee4d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9', 'start_index': 0}, page_content='Boundary Resetting. In settings where multiple independent sequences are stitched together, Transformers can keep\\nthem separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences.\\nSelective SSMs can also reset their state at boundaries (e.g. Î”ğ‘¡ â†’âˆ, or Theorem 1 when ğ‘”ğ‘¡ â†’1). These settings may\\noccur artificially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in\\nreinforcement learning (Lu et al. 2023)).\\nAdditionally, we elaborate on effects of each selective parameter.\\nInterpretation of Î”. In general, Î” controls the balance between how much to focus or ignore the current input ğ‘¥ğ‘¡. It\\ngeneralizes RNN gates (e.g. ğ‘”ğ‘¡ in Theorem 1): mechanically, a large Î” resets the state â„and focuses on the current input ğ‘¥,\\nwhile a small Î” persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system'),\n",
       "  Document(id='b0121721-71a9-480e-a649-7249a6c1c6fa', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='Mamba: Linear-Time Sequence Modeling with Selective State Spaces\\nAlbert Guâˆ—1\\nand Tri Daoâˆ—2\\n1\\nMachine Learning Department, Carnegie Mellon University\\n2\\nDepartment of Computer Science, Princeton University\\nagu@cs.cmu.edu, tri@tridao.me\\nAbstract\\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\\nTransformersâ€™ computational inefficiency on long sequences, but they have not performed as well as attention on important\\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses'),\n",
       "  Document(id='99bf5cec-5082-49b4-a415-1f6f417e1c5c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7', 'start_index': 0}, page_content='3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to\\nrevisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and\\nrecomputation. We make two main observations:\\nâ€¢ The naive recurrent computation usesğ‘‚(ğµğ¿ğ·ğ‘)FLOPs while the convolutional computation usesğ‘‚(ğµğ¿ğ·log(ğ¿))FLOPs,\\nand the former has a lower constant factor. Thus for long sequences and not-too-large state dimension ğ‘, the recurrent\\nmode can actually use fewer FLOPs.\\nâ€¢ The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like\\nthe convolutional mode, we can attempt to not actually materialize the full state â„.\\nThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state â„only in more efficient')],\n",
       " [Document(id='988678e6-2a6d-490f-b5e6-0162ea5df00e', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30', 'start_index': 2426}, page_content='Figure 4 (Left).\\nMamba Architecture: Interleaving Blocks. We test the effect of different architectural blocks combined with the\\nMamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra\\nconv â†’SSM path added. This leads to two natural ablations:\\nâ€¢ What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be\\ninterpreted as taking Mamba and removing half of the SSMs.\\n30'),\n",
       "  Document(id='201ba3fa-bc8d-4158-9ac1-44b2103f00d9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 31, 'page_label': '32', 'start_index': 2424}, page_content='â€¢ HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with\\nthe MHA block replaced by an H3 block using a global convolution parameterized by an MLP.\\nâ€¢ Mamba: the standard Mamba architecture.\\nModel Sizes. We use the following model sizes.\\nBlocks 4 5 6 7 8 10 12\\nModel Dimension 64 96 128 192 256 384 512\\nParams (Approx.) 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M\\nNote that the number of blocks for Mamba is doubled, because one Transformer â€œlayerâ€ includes both the MHA and MLP\\nblocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).\\n32'),\n",
       "  Document(id='5489e1bc-c55a-44d9-af58-56453e1f9f8f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31', 'start_index': 2237}, page_content='/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000049/uni00000004/uni0000000c/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni00000016/uni00000014/uni00000018/uni0000001c/uni0000000d\\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045\\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045/uni0000000f\\n/uni0000002c/uni00000017/uni0000000f\\n/uni0000002c/uni00000017/uni0000000f/uni0000000f\\nFigure 9: (Scaling laws: extra ablations .) (Left) Instead of (Right) Instead of\\nâ€¢ What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking'),\n",
       "  Document(id='42109253-d250-489b-a0e7-c27d867a28a2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 14, 'page_label': '15', 'start_index': 768}, page_content='sequence length 1000, sandwiched on each side by 8 outer blocks on\\nsequence length 4000, sandwiched by 8 outer blocks on sequence\\nlength 16000 (40 blocks total). The architecture of the 8 center\\nblocks are ablated independently of the rest. Note that Transformers\\n(MHA+MLP) were not tested in the more important outer blocks\\nbecause of efficiency constraints.\\nOuter Center NLL â†“ FID â†“ IS â†‘ mIS â†‘ AM â†“\\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\\nMamba MHA+MLP 1.850 1.37 5.63 58.23 0.62\\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\\nMamba Mamba 1.852 0.94 6.26 88.54 0.52\\n4.5 Speed and Memory Benchmarks\\nWe benchmark the speed of the SSM scan operation (state expansion ğ‘ = 16), as well as the end-to-end inference\\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of'),\n",
       "  Document(id='0976947e-a659-4ce8-9978-3bc7ee672eb1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14', 'start_index': 6015}, page_content='Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GAN-\\nand diffusion- based models. A larger model parameter-matched to the baselines further improves on fidelity metrics\\ndramatically.\\nTable 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages and\\ncenter stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba > S4+MLP >\\nMHA+MLP in the center blocks.\\n14'),\n",
       "  Document(id='031d5d0d-db58-4983-9b09-9bc972fe424b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35', 'start_index': 3832}, page_content='ally benefit from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same parameterization)\\n(Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left.\\nis uniformly sampled and very smooth, and therefore benefits from continuous linear time-invariant (LTI) methods.\\nAfter ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To\\ndisambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6.\\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The\\nperformance differences shrink dramatically; this reinforces the hypothesis that layers closer to theraw audio signal should\\nbe LTI, but once they are â€œtokenizedâ€ and compressed by the outer layers, the inner layers no longer need to be LTI. In this')]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#retrievers\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6},\n",
    ")\n",
    "\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"What is the main idea of the document?\",\n",
    "        \"What is the mamba block?\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åé¢è¿˜æœ‰ä¸œè¥¿æ‰å¯¹ï¼Œæˆ‘ä»¬è¦æŠŠvector store æ‹¿åˆ°çš„ä¸œè¥¿æ”¾åˆ°prompté‡Œé¢ ç„¶åç»™åˆ°å¤§æ¨¡å‹å•Š\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "query = \"ä½ çš„é—®é¢˜ï¼Œæ¯”å¦‚ï¼šWhat is the main idea of the document?\"\n",
    "results = vector_store.similarity_search(query, k=4)  # k å–ä½ æƒ³è¦çš„æ•°é‡\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ context å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\\n\\nContext:\\n{context}\"\n",
    "user_prompt = \"Question: {question}\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(system_prompt.format(context=context)),\n",
    "    HumanMessage(user_prompt.format(question=query)),\n",
    "]\n",
    "\n",
    "# TODO: æˆ‘è®°å¾—langchainå¤„ç†è¿™ç§ragæœ‰ä¸“é—¨çš„chainæ¥ç€ï¼Œæ˜å¤©å†çœ‹çœ‹å§\n",
    "def rag_ask(query, vector_store, model, k=4):\n",
    "    results = vector_store.similarity_search(query, k=k)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    system_prompt = \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ context å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\\n\\nContext:\\n{context}\"\n",
    "    user_prompt = \"Question: {question}\"\n",
    "    messages = [\n",
    "        SystemMessage(system_prompt.format(context=context)),\n",
    "        HumanMessage(user_prompt.format(question=query)),\n",
    "    ]\n",
    "    return model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The main idea of the document revolves around **selective state space models (SSMs)** and their advantages over traditional models like Transformers and linear time-invariant (LTI) SSMs. Key points include:\\n\\n1. **Selectivity in SSMs**:  \\n   - The document highlights how introducing selectivity (e.g., through the parameter Î”) allows SSMs to dynamically focus on or ignore inputs, improving performance with minimal parameter overhead.  \\n   - A small Î” retains the state and ignores the current input, while a large Î” resets the state and focuses on the input (similar to RNN gates).  \\n\\n2. **Performance Improvements**:  \\n   - Increasing the state size (ğ‘) in selective SSMs leads to significant perplexity improvements (over 1.0) with only a 1% increase in parameters.  \\n   - Selective SSMs outperform recurrent baselines (e.g., RWKV, RetNet) on long sequences (e.g., 8k context length) due to better efficiency.  \\n\\n3. **Interpretation of Parameters**:  \\n   - **Î”**: Governs input selection and state retention.  \\n   - **ğ‘¨, ğ‘©, ğ‘ª**: Selectivity in Î” is sufficient for performance gains, though making ğ‘¨ selective could also help.  \\n   - **Boundary Resetting**: Selective SSMs can reset states at sequence boundaries (e.g., document or episode boundaries), preventing information leakage.  \\n\\n4. **Comparison to Other Models**:  \\n   - Selective SSMs address limitations of LTI SSMs (e.g., handling varying sequence contexts) and compete with Transformers, especially in long-context settings.  \\n\\n5. **Future Directions**:  \\n   - The document suggests further exploration of selective mechanisms (e.g., in ğ‘¨) and applications in areas like reinforcement learning.  \\n\\nIn summary, the core argument is that **selective SSMs offer a computationally efficient and flexible alternative to Transformers and traditional SSMs, with strong empirical results and interpretable dynamics**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 407, 'prompt_tokens': 667, 'total_tokens': 1074, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 667}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '5de6cc79-4fe2-4eec-a57b-f0adb7111526', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--f4d1cefa-94f0-45b1-bb61-d418e8e951c0-0' usage_metadata={'input_tokens': 667, 'output_tokens': 407, 'total_tokens': 1074, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "answer = rag_ask(\"What is the main idea of the document?\", vector_store, model)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the document revolves around **selective state space models (SSMs)** and their advantages over traditional models like Transformers and linear time-invariant (LTI) SSMs. Key points include:\\n\\n1. **Selectivity in SSMs**:  \\n   - The document highlights how introducing selectivity (e.g., through the parameter Î”) allows SSMs to dynamically focus on or ignore inputs, improving performance with minimal parameter overhead.  \\n   - A small Î” retains the state and ignores the current input, while a large Î” resets the state and focuses on the input (similar to RNN gates).  \\n\\n2. **Performance Improvements**:  \\n   - Increasing the state size (ğ‘) in selective SSMs leads to significant perplexity improvements (over 1.0) with only a 1% increase in parameters.  \\n   - Selective SSMs outperform recurrent baselines (e.g., RWKV, RetNet) on long sequences (e.g., 8k context length) due to better efficiency.  \\n\\n3. **Interpretation of Parameters**:  \\n   - **Î”**: Governs input selection and state retention.  \\n   - **ğ‘¨, ğ‘©, ğ‘ª**: Selectivity in Î” is sufficient for performance gains, though making ğ‘¨ selective could also help.  \\n   - **Boundary Resetting**: Selective SSMs can reset states at sequence boundaries (e.g., document or episode boundaries), preventing information leakage.  \\n\\n4. **Comparison to Other Models**:  \\n   - Selective SSMs address limitations of LTI SSMs (e.g., handling varying sequence contexts) and compete with Transformers, especially in long-context settings.  \\n\\n5. **Future Directions**:  \\n   - The document suggests further exploration of selective mechanisms (e.g., in ğ‘¨) and applications in areas like reinforcement learning.  \\n\\nIn summary, the core argument is that **selective SSMs offer a computationally efficient and flexible alternative to Transformers and traditional SSMs, with strong empirical results and interpretable dynamics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The main idea of the document revolves around **selective state space models (SSMs)** and their advantages over traditional models like Transformers and linear time-invariant (LTI) SSMs. Key points include:\\n\\n1. **Selectivity in SSMs**:  \\n   - The document highlights how introducing selectivity (e.g., through the parameter Î”) allows SSMs to dynamically focus on or ignore inputs, improving performance with minimal parameter overhead.  \\n   - A small Î” retains the state and ignores the current input, while a large Î” resets the state and focuses on the input (similar to RNN gates).  \\n\\n2. **Performance Improvements**:  \\n   - Increasing the state size (ğ‘) in selective SSMs leads to significant perplexity improvements (over 1.0) with only a 1% increase in parameters.  \\n   - Selective SSMs outperform recurrent baselines (e.g., RWKV, RetNet) on long sequences (e.g., 8k context length) due to better efficiency.  \\n\\n3. **Interpretation of Parameters**:  \\n   - **Î”**: Governs input selection and state retention.  \\n   - **ğ‘¨, ğ‘©, ğ‘ª**: Selectivity in Î” is sufficient for performance gains, though making ğ‘¨ selective could also help.  \\n   - **Boundary Resetting**: Selective SSMs can reset states at sequence boundaries (e.g., document or episode boundaries), preventing information leakage.  \\n\\n4. **Comparison to Other Models**:  \\n   - Selective SSMs address limitations of LTI SSMs (e.g., handling varying sequence contexts) and compete with Transformers, especially in long-context settings.  \\n\\n5. **Future Directions**:  \\n   - The document suggests further exploration of selective mechanisms (e.g., in ğ‘¨) and applications in areas like reinforcement learning.  \\n\\nIn summary, the core argument is that **selective SSMs offer a computationally efficient and flexible alternative to Transformers and traditional SSMs, with strong empirical results and interpretable dynamics**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 407, 'prompt_tokens': 667, 'total_tokens': 1074, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 667}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '5de6cc79-4fe2-4eec-a57b-f0adb7111526', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--f4d1cefa-94f0-45b1-bb61-d418e8e951c0-0' usage_metadata={'input_tokens': 667, 'output_tokens': 407, 'total_tokens': 1074, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "answer = rag_ask(\"What is the main idea of the document?\", vector_store, model)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
