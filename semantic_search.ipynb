{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2025/6/24\n",
    "# zhangzhong\n",
    "# https://python.langchain.com/docs/tutorials/retrievers/\n",
    "\n",
    "from utils import load_env\n",
    "load_env()\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
    "\n",
    "# - page_content: a string representing the content;\n",
    "# - metadata: a dict containing arbitrary metadata;\n",
    "# - id: (optional) a string identifier for the document.\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
      "Albert Gu∗1\n",
      "and Tri Dao∗2\n",
      "1\n",
      "Machine {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#loading-documents\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "file_path = \"mamba.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "# PyPDFLoader loads one Document object per PDF page\n",
    "# 还真是，我们的manba论文总共有36页\n",
    "docs: list[Document] = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0].page_content[:100], docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#splitting\n",
    "# We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks\n",
    "# We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size\n",
    "# We set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.\n",
    "\n",
    "# blabla, just give me an example!\n",
    "from typing import List\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 这个类还可以自己指定 seperator 不过默认的seperator应该就是通用的\n",
    "# 可能在处理英文的时候比较合适吧，处理中文的可能需要自己写？或者他是按照段落 空格 换行来做seperator的？\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits: List[Document] = text_splitter.split_documents(documents=docs)\n",
    "len(all_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以看到在metadata里面多了一个 start_index 用于标注这个chunk在原文中的位置\n",
    "# 这样就可以做引用了\n",
    "print(all_splits[0].page_content[:100], all_splits[0].metadata)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "print(all_splits[0].page_content)\n",
    "print(\"----------------------------------\")\n",
    "print(all_splits[1].page_content)\n",
    "print(\"----------------------------------\")\n",
    "print(all_splits[2].page_content)\n",
    "print(\"----------------------------------\")\n",
    "# 我懂了，看输出效果，就是把原来的文本按照某些规则，比如固定的字符数，或者段落，给分开，然后两个相邻的chunk会overlap一段\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#embeddings\n",
    "# https://www.reddit.com/r/LocalLLaMA/comments/1iyun6z/using_deepseek_r1_for_rag_dos_and_donts/\n",
    "# 找到一个好东西，说了用千问的embedding模型和deepseek的reason 模型 可以\n",
    "# 我去看看千问有没有API可以买吧\n",
    "# https://github.com/Graph-RAG/GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "[-0.038538604974746704, 0.03440577909350395, 0.026333853602409363, -0.046597618609666824, -0.05496659129858017, -0.01645381562411785, -0.012773016467690468, 0.044376224279403687, -0.06524699926376343, 0.12873753905296326, 0.004181257914751768, 0.0032949603628367186, -0.011681691743433475, 0.012630950659513474, -0.006935399491339922, -0.012340361252427101, 0.0030915478710085154, -0.0640588104724884, 0.004010132979601622, 0.005624518264085054, -0.04623599350452423, 0.02319548837840557, -0.0238024964928627, 0.009350519627332687, -0.01554975938051939, 0.009763802401721478, -0.0293947272002697, -0.08591112494468689, 0.019953802227973938, -0.008388346061110497, -0.0488448403775692, 0.023789580911397934, 0.007342224474996328, 0.00339020905084908, -0.04357548803091049, 0.038254473358392715, 0.016208428889513016, 0.03921018913388252, -0.016867097467184067, 0.0042619770392775536, -0.036653004586696625, 0.005660034716129303, -0.06571193784475327, 0.025662269443273544, -0.012146634981036186, 0.006877281237393618, 0.009221368469297886, -0.06116583198308945, -0.03130615875124931, 0.064782053232193, -0.04827657714486122, -0.04132826253771782, -0.008698307909071445, -0.03745374083518982, -0.0096152788028121, 0.020341254770755768, 0.009854207746684551, -0.015343117527663708, -0.002461937488988042, 0.02000546269118786, 0.04763082414865494, 0.04915480315685272, -0.034328289330005646, 0.024047883227467537, 0.02585599571466446, -0.04478950425982475, -0.036988794803619385, 0.0060216570273041725, 0.014413231983780861, -0.020625386387109756, 0.009744429960846901, 0.057704586535692215, -0.053881723433732986, -0.01267615333199501, -0.06199239566922188, 0.054604969918727875, -0.04148324579000473, -0.015717655420303345, -0.04143158346414566, 0.009679853916168213, 0.0025781732983887196, -0.024990685284137726, -0.011565456166863441, -0.018416907638311386, 0.025920569896697998, 0.05075627565383911, -0.023440875113010406, -0.0774388313293457, 0.04683008790016174, -0.04117328301072121, -0.0065447180531919, 0.04559024050831795, 0.009473212994635105, -0.0037744329310953617, 0.018713954836130142, -0.006170180626213551, -0.0605459064245224, 0.025533117353916168, 0.013354195281863213, -0.030892876908183098, -0.032623499631881714, -0.0037744329310953617, -0.004952934104949236, -0.03600725159049034, 0.026256361976265907, 0.03528400510549545, -0.060287605971097946, -0.06385216861963272, -0.00922782626003027, 0.0302729532122612, 0.005127287935465574, -0.0018064971081912518, -0.0013916001189500093, 0.0066706398501992226, 0.013328365050256252, -0.017693663015961647, -0.029962990432977676, 0.00024034161469899118, -0.014271166175603867, -0.010299778543412685, 0.02056081034243107, 0.02568809874355793, -0.06571193784475327, -0.035723116248846054, 0.032571837306022644, -0.022730544209480286, 0.025752674788236618, -0.019127236679196358, 0.03515485301613808, 0.01591138169169426, 0.004817325621843338, -0.06064922735095024, 0.03463825210928917, -0.005692322738468647, 0.010932616889476776, -0.02121948078274727, 0.03487072139978409, 0.03319176286458969, 0.01722872070968151, 0.016105107963085175, -0.04236147180199623, 0.0006663375534117222, 0.06488537043333054, 0.01640215516090393, 0.002922037383541465, -0.00484638474881649, -0.01569182425737381, -0.042671430855989456, -0.004917417652904987, -0.02872314304113388, -0.03797034174203873, -0.013354195281863213, -0.01508481614291668, 0.009150335565209389, -0.004613913130015135, -0.054553307592868805, 0.03226187452673912, -0.042335640639066696, 0.015898466110229492, 0.027922408655285835, 0.02043166011571884, -0.03104785829782486, 0.03381168469786644, 0.009821919724345207, -0.03357921540737152, -0.019785907119512558, -0.07661227136850357, -0.02297593094408512, -0.017990710213780403, -0.013883713632822037, -0.012275786139070988, -0.011675234884023666, -0.020134612917900085, 0.051402028650045395, 0.038202814757823944, -0.028748974204063416, -0.010977820493280888, -0.004155427683144808, 0.028025729581713676, 0.05904775857925415, 0.01879144459962845, -0.00609268993139267, 0.14506220817565918, -0.01855897344648838, 0.06886322051286697, 0.0006029729265719652, 0.0033708366099745035, 0.041457414627075195, -0.02524898573756218, 0.03623972088098526, -0.024073714390397072, 0.033450063318014145, 0.032236047089099884, 0.017667831853032112, 0.023285893723368645, 0.027044182643294334, -0.016337579116225243, 0.07061967253684998, 0.0458485409617424, 0.023169657215476036, -0.012482427060604095, -0.037531230598688126, 0.0402175672352314, 0.006735215429216623, 0.05398504436016083, -2.4594151909695938e-05, -0.022498073056340218, -0.06395548582077026, 0.03174527361989021, 0.0023989765904843807, 0.0004447631654329598, -0.004888358525931835, -0.023389214649796486, -0.008381888270378113, 0.022330177947878838, 0.01469736360013485, -0.03967513144016266, 0.0006122556515038013, 0.013573751784861088, -0.02519732527434826, -0.01945011503994465, 0.04047586768865585, -0.0055793155916035175, -0.013844968751072884, -0.008388346061110497, 0.053055159747600555, -0.022691799327731133, 0.022665970027446747, 0.03164195269346237, 0.02823236957192421, -0.02413828857243061, 0.05011051893234253, -0.057652927935123444, -0.04636514559388161, -0.021335715427994728, -0.02319548837840557, -0.026153041049838066, -0.045099467039108276, 0.009511957876384258, 0.00980254728347063, 0.03735041990876198, 0.05142785981297493, -0.029575537890195847, -0.006192781962454319, 0.01904974691569805, -0.016260089352726936, -0.03812532499432564, -0.029808010905981064, 0.04269726201891899, -0.0168929286301136, -0.010177085176110268, -0.023841241374611855, -0.023298809304833412, 0.011894790455698967, -0.015046071261167526, -0.03104785829782486, 0.020186273381114006, 0.02524898573756218, 0.07382261008024216, -0.041457414627075195, -0.02602389082312584, 0.00016890506958588958, 0.028051558881998062, -0.002943024504929781, -0.011094056069850922, -0.00792986061424017, -0.03213272616267204, 0.0261788722127676, -0.06163077428936958, 0.01121674943715334, -0.019656755030155182, 0.025778504088521004, 0.010235202498733997, -0.027431635186076164, 0.0028397038113325834, -0.009260114282369614, -0.01651838980615139, 0.002744455123320222, -0.04608101397752762, 0.05039465054869652, -0.031848594546318054, 0.013741647824645042, -0.04104413092136383, 0.005088542588055134, -0.05408836528658867, 0.011533168144524097, 0.010719518177211285, -0.03623972088098526, -0.0367821529507637, 0.009699227288365364, 0.04827657714486122, -0.018688123673200607, 0.015343117527663708, 0.012385563924908638, 0.040579188615083694, 0.0008237400907091796, 0.0017806669929996133, 0.03497404232621193, 0.02944638766348362, -0.00884037371724844, -0.008827459067106247, -0.02408662810921669, -0.044608693569898605, 0.05414002388715744, 0.007865285500884056, 0.010261032730340958, -0.018261926248669624, -0.01707373932003975, 0.006128206383436918, -0.1265678107738495, -0.0017806669929996133, 0.05827285349369049, 0.024344930425286293, -0.03037627413868904, 0.03804783150553703, -0.007923402823507786, -0.043782129883766174, -0.026333853602409363, 0.03337257355451584, -8.717680611880496e-05, -0.029420558363199234, -0.03032461367547512, -0.002408662810921669, 0.012450139038264751, 0.043730467557907104, -0.02634676732122898, 0.0037550602573901415, -0.024551572278141975, -0.007316394243389368, -0.02187814936041832, -0.045822713524103165, -0.0010880957124754786, 0.020341254770755768, 0.06209571659564972, 0.03094453737139702, 0.02828403003513813, 0.01894642598927021, 0.057084664702415466, -0.05398504436016083, 0.0011938379611819983, 0.03657551482319832, -0.015872636809945107, -0.020082952454686165, 0.03866775706410408, -0.0013657700037583709, 0.024344930425286293, -0.0051046861335635185, 0.011475050821900368, -0.0023327867966145277, 0.008737053722143173, 0.006108833942562342, 0.029808010905981064, 0.0011244193883612752, 0.02353128045797348, 0.014516552910208702, -0.054449986666440964, 0.02993716113269329, 0.00015094502305146307, -0.028154879808425903, -0.005398504436016083, -0.007658643648028374, -0.015536843799054623, -0.043833788484334946, 0.03474157303571701, -0.028413182124495506, -0.02823236957192421, -0.04045003652572632, 0.009537788107991219, -0.017383700236678123, 0.01904974691569805, -0.028154879808425903, -0.05065295472741127, 0.00873059593141079, 0.020018378272652626, 0.02861982211470604, -0.007503662724047899, -0.02083202823996544, -7.602745608892292e-05, 0.029472218826413155, 0.004878672305494547, 0.010713061317801476, 0.010654943063855171, -0.04998137056827545, -0.024990685284137726, 0.021193649619817734, -0.005989369470626116, 0.012798846699297428, 0.08208826184272766, -0.006631894968450069, -0.030092142522335052, 0.02580433525145054, 0.005656806286424398, 0.0014650546945631504, 0.010144797153770924, -0.017745323479175568, 0.00679333321750164, 0.025765588507056236, 0.0033482350409030914, 0.0674167275428772, -0.041354093700647354, -0.005999055691063404, 0.009834835305809975, 0.015304372645914555, -0.003525817533954978, 0.006228298414498568, -0.024164119735360146, 0.021413207054138184, 0.007716761901974678, 0.004023048095405102, -0.04590020328760147, -0.04551275074481964, -0.033010952174663544, 0.05044631287455559, -0.004555795341730118, 0.013225044123828411, 0.0012164392974227667, -0.008536869660019875, -0.017370786517858505, -0.022717630490660667, -0.03081538714468479, 0.0125405453145504, -0.01433574128895998, 0.013128180988132954, -0.05899609625339508, -0.03693713620305061, 0.0712912529706955, -0.03585226833820343, -0.023608770221471786, -0.01717706024646759, 0.05204778164625168, -0.06555695831775665, -0.03618806228041649, 0.012630950659513474, 0.020741622895002365, 0.023079251870512962, 0.0009323075064457953, 0.04088915139436722, -0.03629138320684433, -0.07676725089550018, 0.013857883401215076, 0.03213272616267204, -0.0008935622754506767, -0.025597693398594856, -0.03275264799594879, -0.03735041990876198, 0.012766558676958084, 0.0007510927389375865, 0.020095868036150932, -0.0009121277253143489, 0.014038695022463799, -0.022407667711377144, -0.0017548368778079748, -0.019191812723875046, 0.01717706024646759, 0.013741647824645042, 0.012114346958696842, 0.04817325621843338, -0.034715741872787476, -0.036601342260837555, -0.004010132979601622, 0.0005634204717352986, 0.00836897362023592, -0.018094031140208244, -0.020948262885212898, -0.003068946534767747, 0.05137619748711586, -0.024603232741355896, 0.012023941613733768, -0.028697313740849495, -0.009963986463844776, 0.02905893512070179, 0.01549809891730547, 0.006392966024577618, -0.034999873489141464, -0.007535950746387243, 0.009240741841495037, 0.023156743496656418, 0.03406998887658119, -0.009679853916168213, 0.06700345128774643, 0.017551597207784653, 0.020534981042146683, -0.029420558363199234, 0.011332985013723373, -0.06726174801588058, -0.018429823219776154, -0.030531255528330803, 0.05625809729099274, 0.027044182643294334, 0.023453788831830025, 0.010680773295462132, 0.007445544935762882, 0.0038164069410413504, 0.014232421293854713, -0.03319176286458969, 0.05029132962226868, -0.013961204327642918, 0.01817152090370655, -0.03923602029681206, 0.012727813795208931, -0.00911804847419262, -0.020031291991472244, -0.019075576215982437, -0.042568109929561615, 0.012811761349439621, -0.01121674943715334, -0.0721694827079773, 0.003571020206436515, 0.035723116248846054, -0.04246479272842407, 0.011914163827896118, 0.04419541358947754, 0.045538581907749176, -0.04473784565925598, -0.011397560127079487, 0.0014166231267154217, 0.0162342581897974, 0.06963812559843063, -0.060080964118242264, -0.04336884617805481, -0.0062702726572752, -0.007871742360293865, -0.021865233778953552, 0.012295158579945564, 0.02403496764600277, -0.029756350442767143, -0.026889201253652573, -0.03856443613767624, 0.036317210644483566, 0.005572858266532421, -0.0005605953047052026, 0.0012067529605701566, -0.013405855745077133, 0.010177085176110268, 0.03055708482861519, 0.0596676804125309, 0.05132453888654709, 0.010906786657869816, -0.0375828891992569, 0.017370786517858505, 2.478333772160113e-05, 0.004655887372791767, -0.007710304111242294, -0.012682611122727394, 0.018804360181093216, -0.03396666795015335, 0.024280354380607605, -0.026863371953368187, -0.031409479677677155, -0.02602389082312584, 0.00664481008425355, -0.004704318940639496, 0.02337629906833172, -0.037815362215042114, 0.004746292717754841, -0.022510988637804985, 0.030195463448762894, 0.014025779440999031, -0.04045003652572632, 0.03363087400794029, -0.020909518003463745, -0.025868909433484077, 0.002292427234351635, -0.0428522452712059, -0.012805304490029812, 0.011533168144524097, -0.06225069612264633, -0.013470430858433247, -0.06276729702949524, 0.03406998887658119, -0.056878022849559784, 0.03528400510549545, -0.03280431032180786, 0.020625386387109756, -0.00011633663962129503, -0.006309017539024353, 0.011591286398470402, -0.005304870195686817, 0.017874473705887794, 0.00030289904680103064, -0.026656730100512505, -0.026863371953368187, -0.0024328785948455334, 0.022717630490660667, 0.021025754511356354, 0.03314010053873062, 0.007174328435212374, -0.010706603527069092, -0.01469736360013485, -0.004623599350452423, 0.023505449295043945, -0.023169657215476036, -0.033114273101091385, -0.0060216570273041725, -0.0034483270719647408, 0.04837989807128906, 0.03797034174203873, 0.01613093726336956, 0.004868986085057259, 0.032726820558309555, 0.040295056998729706, -0.03662717342376709, -0.025442712008953094, 0.012915082275867462, -0.017745323479175568, 0.010286862961947918, 0.017202889546751976, 0.028051558881998062, 0.012527629733085632, -0.025701014325022697, 0.1038372591137886, 0.022472243756055832, -0.010512877255678177, 0.004781809169799089, -0.014477807097136974, -0.004494448658078909, -0.020147528499364853, 0.005359759088605642, 0.014038695022463799, -0.02004420757293701, -0.024603232741355896, -0.011726894415915012, 0.014684448949992657, -0.027199164032936096, -0.005333928856998682, -0.021839404478669167, 0.007477832492440939, -0.0026911802124232054, -0.04866402968764305, 0.0008879119413904846, -0.07010306417942047, 0.0614241324365139, -0.018494397401809692, 0.022640138864517212, 0.04623599350452423, -0.019514689221978188, -0.05744628608226776, 0.005569629371166229, -0.015536843799054623, 0.013715817593038082, -0.005634204484522343, -0.0003727212024386972, -0.06679680943489075, -0.028903953731060028, 0.03171944245696068, 0.011171545833349228, 0.0003727212024386972, -0.017487021163105965, 0.014826514758169651, -0.0009613664587959647, 0.013677072711288929, -0.005708466283977032, -0.008582072332501411, -0.00033094899845309556, 0.07449419796466827, 0.017745323479175568, 0.018055284395813942, -0.06628020107746124, -0.011991653591394424, 0.009944613091647625, -0.05636141821742058, -0.03324342146515846, -0.024112459272146225, -0.007484290283173323, 0.034328289330005646, 0.03497404232621193, -0.009156793355941772, 0.000653826049529016, -0.01184958778321743, -0.004481533542275429, -0.0476049929857254, -0.003270744578912854, -0.023014677688479424, 0.025765588507056236, 0.07196284085512161, 0.045667730271816254, -0.02552020363509655, -0.011655861511826515, 0.05811787024140358, 0.012876337394118309, 0.0021148447412997484, 0.006722300313413143, 0.001772595103830099, 0.06364552676677704, -0.04055335745215416, 0.005740754306316376, 0.004449245985597372, -0.026062635704874992, 0.01002856157720089, 0.02568809874355793, 0.03900354728102684, -0.021232396364212036, -0.012753644026815891, 0.00544370710849762, -0.03683381527662277, 0.028929784893989563, -0.007645728997886181, -0.014658618718385696, 0.05321013927459717, 0.0484057292342186, -0.05199612304568291, 0.030040482059121132, -0.09340187907218933, -0.039365172386169434, 0.028490671887993813, -0.04522861912846565, 0.00445893220603466, -0.003942328970879316, -0.010809924453496933, -0.018261926248669624, 0.02960136905312538, -0.025326477363705635, -0.039365172386169434, -0.0002504315343685448, 0.0020018378272652626, 0.07563072443008423, 0.05889277532696724, 0.0244611669331789, 0.006825621239840984, 0.01955343410372734, 0.0016531305154785514, -0.04987804964184761, -0.0015199438203126192, -0.03252017870545387, -0.10182251036167145, -0.018442736938595772, 0.027974069118499756, -0.0036162231117486954, 0.026269277557730675, 0.020534981042146683, -0.03419913724064827, -0.06994808465242386, 0.04326552525162697, 0.002416734816506505, 0.017112484201788902, 0.021258225664496422, -0.021916894242167473, -0.0008854903280735016, 0.0027654420118778944, 0.00169833330437541, 0.007535950746387243, 0.0014763553626835346, 0.008678935468196869, -0.02479695901274681, 0.0925753116607666, 0.04517695680260658, -0.003593621775507927, 0.015020241029560566, -0.07878199964761734, -0.0527968555688858, 0.02612721174955368, -0.01894642598927021, 0.03724709898233414, -0.01088095735758543, -0.000986389466561377, -0.01657005026936531, 0.008052553981542587, 0.039184361696243286, 0.022291433066129684, -0.01220475323498249, 0.03138364851474762, 0.002029282273724675, -0.013909543864428997, 0.017319126054644585, -0.022472243756055832, 0.04861237108707428, 0.01044184435158968, -0.0188301894813776, -0.04812159761786461, 0.05661972239613533, -0.03347589448094368, -0.021129075437784195, -0.0035742491018027067, 0.03884856775403023, -0.023686261847615242, -0.013057148084044456, 0.023324638605117798, 0.00809129886329174, -0.010900329798460007, -0.019308047369122505, -0.004998136777430773, -0.02066413126885891, -0.06891488283872604, -0.006034572143107653, 0.026553409174084663, 0.009079302661120892, -0.02761244587600231, 0.008013809099793434, 0.006360678002238274, -0.0384611152112484, -0.018042370676994324, -0.02337629906833172, 0.036162231117486954, -0.0005117601249366999, -0.001175272511318326, -0.04633931443095207, 0.00930531695485115, 0.013302534818649292, 0.0614241324365139, -0.011617116630077362, 0.020625386387109756, -0.03629138320684433, 0.002746069338172674, -0.019398454576730728, 0.02663090080022812, 0.012663238681852818, -0.030479595065116882, 0.030195463448762894, -0.05584481731057167, -0.031022027134895325, -0.019153067842125893, -0.04680426046252251, 0.014296996407210827, -0.01668628677725792, 0.019398454576730728, -0.0006033764802850783, -0.0328301377594471, 0.01210143230855465, 0.00444601709023118, -0.0675717145204544, 0.006702927872538567, 0.004287807270884514, -0.046597618609666824, -0.014529467560350895, -0.012172465212643147, -0.017913218587636948, -0.02635968290269375, 0.013302534818649292, -0.0017467648722231388, 0.013806222938001156, -0.03569728881120682, 0.03585226833820343, -0.02745746448636055, 0.013237959705293179, -0.024538656696677208, 0.025507288053631783, -0.008375431410968304, -0.026863371953368187, 0.025546032935380936, 0.012404936365783215, -0.023105083033442497, -0.03081538714468479, 0.014555297791957855, 0.037531230598688126, -0.01762908697128296, -0.001168814953416586, 0.009266572073101997, -0.0013463973300531507, -0.011927078478038311, 0.003645282005891204, -0.08668603003025055, -0.002936566947028041, -0.0005153925158083439, 0.00455902423709631, 0.03076372668147087, 0.021568188443779945, 0.030453763902187347, 0.03662717342376709, 0.013186299242079258, -0.012785932049155235, -0.004371755290776491, -0.016983333975076675, 0.042335640639066696, 0.0462876558303833, -0.03972679376602173, 0.01557558961212635, 0.03032461367547512, -0.02872314304113388, -0.08260487020015717, 0.019269302487373352, -0.027354145422577858, 0.033062610775232315, 0.01251471508294344, 0.03148696944117546, 0.03812532499432564, 0.003696942236274481, 0.007464917842298746, 0.012682611122727394, 0.025391051545739174, 0.009712141938507557, -0.03174527361989021, 0.012004569172859192, 0.017047908157110214, -0.007503662724047899, 0.016544220969080925, -0.009679853916168213, -0.0009734743507578969, 0.01469736360013485, -0.020638301968574524, 0.0017742094350978732, -0.0073551395907998085, -0.014710279181599617, 0.02280803583562374, 0.0721694827079773, 0.012004569172859192, -0.05827285349369049, -0.03259766846895218, -0.019036831334233284, -0.008291482925415039, 0.004287807270884514, -0.022110620513558388, -0.028206540271639824, -0.007891115732491016, 0.0013754562241956592, 0.046158503741025925, -0.03130615875124931, -0.02061247080564499, -0.0009750887402333319, -0.024512825533747673, -0.048973992466926575, 0.023389214649796486, -0.02779325656592846, -0.006008741911500692, 0.00444601709023118, 0.010293320752680302, -0.02823236957192421, -0.025003599002957344, -0.01088741421699524, 0.006237984634935856, 0.006974144373089075, 0.012999030761420727, 0.037427909672260284, -0.017654918134212494, 0.026140127331018448, -0.030634574592113495, 0.031951915472745895, 0.010919702239334583, 0.03174527361989021, -0.018713954836130142, -0.030892876908183098, -0.05553485453128815, 0.05243523418903351, -0.018649378791451454, 0.006008741911500692, 0.05563817545771599, 0.03895188868045807, -0.013237959705293179, -0.04039837792515755, 0.010299778543412685, 0.001699947752058506, -0.02469363808631897, 0.0054049622267484665, 0.04125077277421951, -0.006857908796519041, 0.01988922618329525, -0.012023941613733768, 0.02464197762310505, 0.02496485412120819, -0.020070038735866547, -0.02663090080022812, -0.05527655407786369, -0.06467873603105545, 0.04484116658568382, -0.028439011424779892, 0.029472218826413155, -0.0253006462007761, -0.003454784629866481, -0.03468991070985794, -0.05512157082557678, -0.026475919410586357, 0.0410957932472229, -0.00719370087608695, -0.01833941787481308, -0.027224993333220482, 0.0021891065407544374, 0.017758239060640335, -0.018817275762557983, 0.034664079546928406, 0.010919702239334583, -0.04243896156549454, 0.01311526633799076, 0.0376603789627552, -0.001769366324879229, -0.03536149486899376, -0.07542408257722855, 0.0023182572331279516, 0.008336685597896576, -0.05661972239613533, -0.04187069833278656, -0.027276653796434402, 0.018029455095529556, 0.03381168469786644, -0.006028114818036556, 0.019966717809438705, -0.023130912333726883, -0.026333853602409363, 0.00021067728812340647, -0.016815437003970146, -0.028258200734853745, 0.01977299153804779, 0.01762908697128296, 0.011468593031167984, -0.012895709834992886, 0.04442788287997246, 0.05209944397211075, 0.026966692879796028, 0.006525345612317324, 0.017758239060640335, -0.010971362702548504, 0.000983160687610507, -0.035826437175273895, 0.010248118080198765, -0.03481906279921532, -0.021748999133706093, -0.04530610889196396, -0.011106970719993114, -0.079556904733181, 0.007949233055114746, -0.02668255940079689, 0.004726920276880264, 0.003015671856701374, 0.011772098019719124, -0.006121749058365822, -0.04352382943034172, 0.06767503172159195, 0.06385216861963272, -0.023905817419290543, 0.017861558124423027, -0.02049623616039753, -0.02597223035991192, -0.023828327655792236, 0.028800634667277336, -0.014619872905313969, -0.013341279700398445]\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/integrations/text_embedding/dashscope/\n",
    "# 找了半天，终于找到了这个\n",
    "# https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2842587.html\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v4\"\n",
    ")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(len(query_result)) # 默认是1024维的向量\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "['b0121721-71a9-480e-a649-7249a6c1c6fa', '45f10c2b-6f32-4280-8bd5-b4e3154505f5', 'f177c9ea-6b26-4462-ba91-58e7c51a85ae', 'b1121ab5-87a1-46a1-888b-468fdbb2e990', 'be8e3550-7e03-4ace-9d3f-233fcc5e760b', 'cbac5b4e-782a-4861-bc24-7698e84aa450', '7faff6b0-04bd-48df-814b-6c631aecde18', 'defba535-b968-4c69-b5e0-45c0d4687df7', '909ed835-bfee-4429-8ddd-a5588f2f2a52', 'e89ef323-423e-423c-83e3-3a9287dca718', '6282ef9d-e33e-46ec-a32c-eb19cb517f7f', '8e26655f-6544-4000-a516-d37edc0977cb', '8bc666a1-76a2-4c9c-a76e-dc50dbb0e0f2', '0c7e423a-de6d-4f0c-92d5-d37bedfd73c4', 'f7f54e86-62dd-422e-8b55-5d5149ec242c', 'a39a658d-6608-4221-bfb4-caede0c70d29', '5c475ff8-5f38-4aab-a2a0-b522b8715d00', 'bc5618de-831b-4db8-8745-efe4dfb2d0fe', '922e6227-01d8-449a-8838-95047219825d', 'a3286e14-1354-42b3-a0ef-da3de069cbbc', 'bb6421b1-e18b-4541-9cb2-965803243e19', '74fbbbd3-909d-4605-9819-fc047d82debe', '7b7be6fe-8c63-44ad-bd4b-93fb2a6e4ca3', '855d54ae-70a8-49b5-a774-1c5f51349c3c', 'c279cbe9-cdc1-4f5b-a78b-49932cf8f1c3', '0ac0e9bc-fc8b-4592-8465-44366f3416fe', '26dcefca-d77d-475d-bfc1-9f6c3861bb03', '7469b255-e53b-4a94-a7ed-dcaa13518594', '105b0644-9950-4017-93b9-d21d8a287d74', 'a707a60f-3a3f-4d60-bd83-c7d1110c2293', 'f77a3d45-1942-4b7e-910e-7838f9a251ca', '99bf5cec-5082-49b4-a415-1f6f417e1c5c', 'cd9eead8-d014-4dd5-ba2f-0c8e5a58c245', '7fb38c17-fa7d-46a9-abef-23754630a124', 'fc5f2c4e-0af6-49e9-97a7-67e70904e971', 'f16fbfd3-7b5f-4a5d-bd8e-fd352a1e7b00', 'f32d4498-58d2-48d8-8e7e-3bcd7f4b29f0', '32e6f2e5-d080-43b4-a2f5-e3f65e672668', '53f9c0ea-e771-4543-8e06-80e1ea352565', 'c7bd32ff-2a11-4511-88fa-ede1a5855ea1', 'd235e606-8f56-468c-b918-524d7995db5a', 'ad4bb55a-72be-4635-9d6a-314ba9cfee4d', '08763aaf-fe4b-45ec-a536-bc265b11042e', 'e4e5472b-f75c-402f-97c7-27a515fc1aab', 'f72bd988-a512-45f5-9d61-aebb8748d40c', 'cbafe45a-4f8d-42d2-9ea0-ead5a4330027', 'a253ba1d-b046-4a44-aec1-fcafe1a0c872', '2cd12120-8534-4bee-9bca-1e26d1933c28', '6502e205-a7e4-4b3a-ae2e-87ff0e6dc620', '873c8d8e-8899-4890-b86d-706a32c9b295', '3fca998b-0907-49b9-987c-71257dce511b', '26b41ebd-e5dd-4e69-954c-450976a1e7a2', '6494d246-ff13-4c42-85f6-86fde353bfb8', '798da949-f39d-46bf-8fcc-4321e85d4ebb', '98116e8b-a600-46b1-bd8a-bea6168d2985', '08cb2c90-c85a-417b-bf63-15bc59881627', '24d7c986-d9ba-403e-8677-1acfc1c7985c', '6d458c4f-32aa-4cef-a6bd-f1eab2dd0bd9', 'f0ef0d14-33ef-4611-82e3-1d1db17a3e5d', '40be6a84-0d09-4b9f-9ede-300b655eabad', '5cdbd6b4-9de6-4e42-91f8-7dce3547e7f2', 'ea9c2372-3320-472f-85d0-2819186c28f6', 'fe73815d-1424-4ff3-bd6c-5db865c542ff', '44313cac-3371-483c-b4f3-9d6f64e1e0d3', 'b630ff48-f3d9-4b2e-b845-274156004e21', 'fb0a03f4-0a84-438e-8b6b-b130af66ef04', '8397b2f2-0324-42df-a645-baa38ff90fd9', '5ef7d50d-72d3-4fe9-9a05-9310f1e0de5e', '49e92d34-fdf4-4c0b-a8d6-7cb57c424c50', 'c6ae7b42-4bb9-4491-8301-ebb34eaf59af', '04d06bc6-eb0e-4bc2-b13e-1c6af4aa107d', '23a8bef7-d155-4a24-b7d4-0384b1a51fb6', '1ffb5075-5055-4852-91b0-e80e31485db1', '650cd7d3-f4bd-4334-ba9a-7e97f040e43b', '08650daa-aa85-4e8c-8025-1d21ac908b03', '4606ab40-7cdf-40be-8a20-b6dc323cbe47', '7aa931d0-6633-429d-bb0e-559c5e63a843', 'c2035bab-eb11-492e-b1a7-583154893fdf', '7704af5e-4739-484b-a4a8-0be453a9451a', '62b8d593-f326-4913-9005-4e1cded82219', '9d9d335a-7776-4ea4-b630-1463e492c0cb', 'a9fc76f1-d831-4b23-b621-2fb256b717dc', 'f8bad5b4-aebe-4fad-8a17-23e86d46af80', '54fa3ece-b1c1-4c73-a12d-e06ed19a83a6', '0976947e-a659-4ce8-9978-3bc7ee672eb1', '10016250-7ffc-4b98-8fa4-d1f249d80729', '42109253-d250-489b-a0e7-c27d867a28a2', 'f143b8aa-5b89-479b-851c-ebe80ba5cb36', 'ac710d3e-af2f-4c9e-a15f-47d739441525', '062db188-e413-4309-8287-13de02f475a7', 'dd9187af-6721-40ea-82ce-04a57aebe2ff', 'ad31112d-fcef-4543-b773-ffce4fea994d', 'fd4f428a-e60f-4547-bc22-262e3d8cee36', 'd61b3f22-7150-46e6-b088-116c44d77d1a', 'd79c87b9-f246-4ed0-bb71-84d1091fb822', 'a2cfe922-7972-461a-bcff-d1482916eb9a', '645f92a0-0207-4391-82cc-4b77f060b699', 'd42e3071-2f1a-4052-bab4-f6da49c17189', 'cac25cba-061a-4668-9051-d7cac5db707e', 'c38f7979-17f9-46aa-a412-3e094f7088c0', 'cd540264-7212-48b9-b82f-aa44ac52fe4f', '0769ce6b-77d6-49a1-8046-0ac18f3033c8', '933bea62-c916-4406-b9f7-b38830bdf4be', 'a12242e5-9d57-4ab1-92ff-fabf2146fa3d', '6d6b5fb4-3903-404f-ba8f-588d02f8e397', '9e21c6e0-1e2b-4190-b735-ab9c54a6e22e', '3ce701e1-9da3-43cd-b98b-2a30e3975077', '85eb8f3a-91ad-4106-8ae9-92545ef37a1b', '9cdbffc3-8b53-419c-972e-ff1e6a999c6a', 'd98bba7f-a873-41be-bc0e-c3ad49add594', '7b60486c-f1f4-4ecc-8249-6c6a1d3c64af', 'ad2fc77d-157a-4145-a563-60e5481a18dc', '4bb3fb37-6001-48b5-ae7a-ba91243ed998', '6d4f17ff-c10f-4f7f-9ee0-daef3b82e540', '52b625cc-199c-4d5d-8754-eb457c0769cd', '7d04311f-2fb9-4525-a74d-d8795a4b2290', '9053edc6-486b-412d-8e96-9eb1eb4b747b', '7e44b13d-f1d8-41ac-bac0-ccd659a74d58', '51608376-9f1c-4029-bb32-89325e283dd1', '8ee9bae4-378a-4440-873d-55b053a142bf', '69fa6b75-4728-4381-a2f5-6d1ec72ad0cb', '2ebc5a06-c324-4ebf-a451-e7f3eddb3818', '0fc6fdf4-4a1c-46c0-9a23-55c66a049123', 'e8496370-8023-4f0d-a667-ddb951021152', '34201dc1-1ba5-478d-ad80-9ae9d40403b7', '7312a51b-ff9e-464e-8f22-19b6093d4f81', '9fab4823-7816-41eb-a90e-537fb425b301', '74be3c36-3e70-4eda-b262-0b7b4068794e', '7a2dedf2-9935-475b-b0ce-c827d2d0a1ba', '75845b3f-16d6-4fbd-bd63-b202c6cb9f49', '3ed59bee-f1bb-483d-94d4-bb421ef8aeff', '8c1d1806-35e4-4cd5-b6cd-ae569fbf1332', '9e72ecd0-7bbd-4cc8-9733-dcf65d7d0eb5', '335d2709-feec-4ddc-9cac-c05671955ae1', '87a3e781-25d3-4beb-b680-d99ce544929e', 'ea405b24-08e2-44a3-b47e-5ab2e567b7dd', '3c8622de-893c-4db6-9174-d48dd667ab3d', 'c68fc0ca-bdaf-42b2-81ae-712485b4d041', '1a8bedd1-c5b7-4956-9cdd-d158d54f198b', '970df476-c387-4955-8177-bd247fe942fe', 'a32a6289-1f3c-47ca-8780-8839c03486d4', '0ecb57ff-7dfd-45fd-a32e-847cac4c2682', '687a566b-f0cb-4c24-92b6-2742e5b9d629', '9f5ff685-9d0a-4bc7-b3b2-1f13264cc67d', '3bb81c18-5db1-450a-a239-3e053aedb4a0', '7c786f4f-3c6c-4fc9-a464-dfba16432498', 'bc9906ae-0a7d-46e5-9cdf-0098018e3259', '7781bdf2-dd4b-4aaf-944d-5e9af79dac2a', 'f31e823d-fbb4-4534-83e3-0b49bd07fbc4', '5356fbe6-944c-46d6-b39d-72ca4e8dc56a', 'e4856133-bc0e-44ac-9e0b-4b834b15338c', '1eea59ef-fdef-44fc-8691-b7b6d4ae47f1', '84e18457-bdc2-4cb6-8ebc-d40424071300', '690e3461-9411-470e-9349-216b23c7102c', '08dbd936-ea17-4427-a22a-a6ff1c91ef15', 'c1d6cce0-2ba1-4f46-a2b3-21303ad79ab3', 'eb2204f8-0064-4a3a-83f4-c07182999ef6', 'c4e6ab74-7af8-457a-bf96-a30ab2a12981', '6ced28d2-af89-40d7-8b17-bf99535bcd12', '7d96fc15-c992-4981-bfed-c2fe622f09ae', '00c828e1-73e9-4e47-82fe-14d6c9d16eae', '7904baa3-1593-411c-9016-b8b4d06722b0', 'f3a19b63-6d7a-4a81-b206-9edffdfd0cf3', '8e7005c7-b5c3-4ee0-b855-ddaf71df11d9', 'e913fd2f-260a-499c-beca-8072c026e140', '3da1da55-682e-4a4c-85f4-1ec0c7b616c3', '0f19898f-9b37-43fe-836b-67f4140151e7', 'b42bc9ab-ce5d-4b92-b776-7d92c60a9db0', '1b7a8ac5-de7d-4cc6-8673-4d464c2ca763', 'a4bf5dd3-688b-4fd5-9c95-f8d3b01069b3', '4b66c2dd-c334-4c54-bb2d-7ee037e446b5', '988678e6-2a6d-490f-b5e6-0162ea5df00e', '8cc02c94-f0be-48b4-ac57-c03ce09be71a', '42f3f78c-ed07-4f11-8086-9810b6b0e833', 'e0422e54-bad7-4de8-a893-c83156fe240e', '5489e1bc-c55a-44d9-af58-56453e1f9f8f', '71135b50-e96b-4a09-b00f-d52a72be7fad', '9ef17096-9728-4235-9172-c9dea52f648e', '8c0b065f-23f9-4f8f-9a4a-dbd73207e289', 'd41d8119-db6f-48a2-9c36-583e5c2ad779', '629d5894-de16-4683-a8dd-17d9af028a0f', '46aa4968-951c-439b-8f9e-f89e6e7edd41', '02507d81-3778-4788-b1bd-517919baa42d', '201ba3fa-bc8d-4158-9ac1-44b2103f00d9', 'efea346a-094d-4d41-ac2d-680311c95d45', 'd8e2d00d-aecf-47b1-87fb-6e23ace58350', 'edd0970e-940c-44b9-ab28-1dfcceb48bb5', 'b10db90b-6335-4c34-87a2-9ef770484e71', '82105a4d-7dcf-4e37-8ddc-04fab09a4c5e', '333acb58-91b2-419d-9c17-556c5ba9e1d9', '49058353-c623-4860-80ff-25b88519b16a', 'f742e875-7296-42e3-a01f-0e9829c828dd', '0c03ef31-6076-4b6d-bc78-4fa84e3848bc', 'e92b4e2d-6569-48ca-a264-18e6b7faeab1', '86236105-048c-4664-85f2-3973b2d0e023', 'd56f41ea-78f0-4b32-b51a-165024f1523e', '9934d23d-9bea-4718-958e-929405b78814', '3cabf40b-90ba-4745-ba3f-991bf0e9e7ab', 'ec4fca7b-734c-4dfc-b350-bec8b205fd12', '031d5d0d-db58-4983-9b09-9bc972fe424b', 'c44538ac-2a26-4828-8f8d-7c6207a9637c', '48b1933d-8753-43b2-a30c-4043ae471a96', '4b07cac3-d725-413b-9bc5-07cb149ab65f', '527650cb-2d4c-4348-a74a-d4e196c27a88', '8b539f26-2601-42d9-b5c3-7abaf9164d2b', '0a1b4e9e-e41d-4247-b162-1b4f9934d9a4']\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#vector-stores\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# create vector store by passing in the embeddings model\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# add documents to the vector store\n",
    "# qwen embedding batch size should not large than 10\n",
    "# 所以这里需要分批处理\n",
    "ids = []\n",
    "for i in range(0, len(all_splits), 10):\n",
    "    ids.extend(vector_store.add_documents(documents=all_splits[i:i+10]))\n",
    "\n",
    "\n",
    "print(len(ids))\n",
    "print(ids)\n",
    "# 这里ids的长度和all splits的长度是一样的\n",
    "# 也就是每个document都是返回一个id\n",
    "# 那么这个id是干嘛的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "modestly with a small increase in parameter count.\n",
      "Of particular note is the dramatic improvement of the selective SSM when the state size 𝑁 is increased, with over a 1.0\n",
      "perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1\n",
      "and 3.3.\n",
      "5 Discussion\n",
      "We discuss related work, limitations, and some future directions.\n",
      "Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an\n",
      "extended related work of SSMs and other related models.\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\"What is the main idea of the document?\")\n",
    "print(len(results))\n",
    "# 这不对吧，这返回的东西和我们的问题完全没联系啊。。。\n",
    "print(results[0].page_content)\n",
    "\n",
    "\n",
    "# 还有async的\n",
    "# results = await vector_store.asimilarity_search(\"When was Nike incorporated?\")\n",
    "\n",
    "# 还可以通过embedding进行搜索\n",
    "# 其实我们用文本进行搜索 本质上再vector store里面还是会调用embedding模型 然后再在向量数据库里面做搜索\n",
    "# embedding = embeddings.embed_query(\"How were Nike's margins impacted in 2023?\")\n",
    "# results = vector_store.similarity_search_by_vector(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='c38f7979-17f9-46aa-a412-3e094f7088c0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 15, 'page_label': '16', 'start_index': 2440}, page_content='modestly with a small increase in parameter count.\\nOf particular note is the dramatic improvement of the selective SSM when the state size 𝑁 is increased, with over a 1.0\\nperplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1\\nand 3.3.\\n5 Discussion\\nWe discuss related work, limitations, and some future directions.\\nRelated Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an\\nextended related work of SSMs and other related models.\\n16'),\n",
       "  Document(id='fe73815d-1424-4ff3-bd6c-5db865c542ff', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 10, 'page_label': '11', 'start_index': 7771}, page_content='recipe (Transformer++) that has now become standard, particularly as the sequence length grows. (We note\\nthat full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that\\ncan also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealistic\\ncomputation requirements.)\\n11'),\n",
       "  Document(id='08763aaf-fe4b-45ec-a536-bc265b11042e', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9', 'start_index': 827}, page_content='while a small Δ persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system\\ndiscretized by a timestep Δ, and in this context the intuition is that large Δ →∞ represents the system focusing on the\\ncurrent input for longer (thus “selecting” it and forgetting its current state) while a small Δ →0 represents a transient\\ninput that is ignored.\\nInterpretation of 𝑨. We remark that while the 𝑨 parameter could also be selective, it ultimately affects the model\\nonly through its interaction with Δ via 𝑨 = exp(Δ𝑨)(the discretization (4)). Thus selectivity in Δ is enough to ensure\\nselectivity in (𝑨,𝑩), and is the main source of improvement. We hypothesize that making 𝑨 selective in addition to (or\\ninstead of) Δ would have similar performance, and leave it out for simplicity.\\nInterpretation of 𝑩 and 𝑪. As discussed in Section 3.1, the most important property of selectivity is filtering out'),\n",
       "  Document(id='ad4bb55a-72be-4635-9d6a-314ba9cfee4d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 8, 'page_label': '9', 'start_index': 0}, page_content='Boundary Resetting. In settings where multiple independent sequences are stitched together, Transformers can keep\\nthem separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences.\\nSelective SSMs can also reset their state at boundaries (e.g. Δ𝑡 →∞, or Theorem 1 when 𝑔𝑡 →1). These settings may\\noccur artificially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in\\nreinforcement learning (Lu et al. 2023)).\\nAdditionally, we elaborate on effects of each selective parameter.\\nInterpretation of Δ. In general, Δ controls the balance between how much to focus or ignore the current input 𝑥𝑡. It\\ngeneralizes RNN gates (e.g. 𝑔𝑡 in Theorem 1): mechanically, a large Δ resets the state ℎand focuses on the current input 𝑥,\\nwhile a small Δ persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system'),\n",
       "  Document(id='b0121721-71a9-480e-a649-7249a6c1c6fa', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='Mamba: Linear-Time Sequence Modeling with Selective State Spaces\\nAlbert Gu∗1\\nand Tri Dao∗2\\n1\\nMachine Learning Department, Carnegie Mellon University\\n2\\nDepartment of Computer Science, Princeton University\\nagu@cs.cmu.edu, tri@tridao.me\\nAbstract\\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\\nTransformers’ computational inefficiency on long sequences, but they have not performed as well as attention on important\\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses'),\n",
       "  Document(id='99bf5cec-5082-49b4-a415-1f6f417e1c5c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 6, 'page_label': '7', 'start_index': 0}, page_content='3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to\\nrevisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and\\nrecomputation. We make two main observations:\\n• The naive recurrent computation uses𝑂(𝐵𝐿𝐷𝑁)FLOPs while the convolutional computation uses𝑂(𝐵𝐿𝐷log(𝐿))FLOPs,\\nand the former has a lower constant factor. Thus for long sequences and not-too-large state dimension 𝑁, the recurrent\\nmode can actually use fewer FLOPs.\\n• The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like\\nthe convolutional mode, we can attempt to not actually materialize the full state ℎ.\\nThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state ℎonly in more efficient')],\n",
       " [Document(id='988678e6-2a6d-490f-b5e6-0162ea5df00e', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 29, 'page_label': '30', 'start_index': 2426}, page_content='Figure 4 (Left).\\nMamba Architecture: Interleaving Blocks. We test the effect of different architectural blocks combined with the\\nMamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra\\nconv →SSM path added. This leads to two natural ablations:\\n• What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be\\ninterpreted as taking Mamba and removing half of the SSMs.\\n30'),\n",
       "  Document(id='201ba3fa-bc8d-4158-9ac1-44b2103f00d9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 31, 'page_label': '32', 'start_index': 2424}, page_content='• HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with\\nthe MHA block replaced by an H3 block using a global convolution parameterized by an MLP.\\n• Mamba: the standard Mamba architecture.\\nModel Sizes. We use the following model sizes.\\nBlocks 4 5 6 7 8 10 12\\nModel Dimension 64 96 128 192 256 384 512\\nParams (Approx.) 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M\\nNote that the number of blocks for Mamba is doubled, because one Transformer “layer” includes both the MHA and MLP\\nblocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).\\n32'),\n",
       "  Document(id='5489e1bc-c55a-44d9-af58-56453e1f9f8f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 30, 'page_label': '31', 'start_index': 2237}, page_content='/uni00000037/uni00000047/uni00000045/uni00000050/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000030/uni00000045/uni0000005b/uni00000057/uni00000004/uni00000053/uni00000052/uni00000004/uni00000038/uni0000004c/uni00000049/uni00000004/uni00000034/uni0000004d/uni00000050/uni00000049/uni00000004/uni0000000c/uni00000037/uni00000049/uni00000055/uni00000059/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni00000030/uni00000049/uni00000052/uni0000004b/uni00000058/uni0000004c/uni00000004/uni00000016/uni00000014/uni00000018/uni0000001c/uni0000000d\\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045\\n/uni0000002c/uni0000005d/uni00000049/uni00000052/uni00000045/uni0000000f\\n/uni0000002c/uni00000017/uni0000000f\\n/uni0000002c/uni00000017/uni0000000f/uni0000000f\\nFigure 9: (Scaling laws: extra ablations .) (Left) Instead of (Right) Instead of\\n• What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking'),\n",
       "  Document(id='42109253-d250-489b-a0e7-c27d867a28a2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 14, 'page_label': '15', 'start_index': 768}, page_content='sequence length 1000, sandwiched on each side by 8 outer blocks on\\nsequence length 4000, sandwiched by 8 outer blocks on sequence\\nlength 16000 (40 blocks total). The architecture of the 8 center\\nblocks are ablated independently of the rest. Note that Transformers\\n(MHA+MLP) were not tested in the more important outer blocks\\nbecause of efficiency constraints.\\nOuter Center NLL ↓ FID ↓ IS ↑ mIS ↑ AM ↓\\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\\nMamba MHA+MLP 1.850 1.37 5.63 58.23 0.62\\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\\nMamba Mamba 1.852 0.94 6.26 88.54 0.52\\n4.5 Speed and Memory Benchmarks\\nWe benchmark the speed of the SSM scan operation (state expansion 𝑁 = 16), as well as the end-to-end inference\\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of'),\n",
       "  Document(id='0976947e-a659-4ce8-9978-3bc7ee672eb1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 13, 'page_label': '14', 'start_index': 6015}, page_content='Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GAN-\\nand diffusion- based models. A larger model parameter-matched to the baselines further improves on fidelity metrics\\ndramatically.\\nTable 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages and\\ncenter stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba > S4+MLP >\\nMHA+MLP in the center blocks.\\n14'),\n",
       "  Document(id='031d5d0d-db58-4983-9b09-9bc972fe424b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-06-03T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-06-03T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'mamba.pdf', 'total_pages': 36, 'page': 34, 'page_label': '35', 'start_index': 3832}, page_content='ally benefit from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same parameterization)\\n(Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left.\\nis uniformly sampled and very smooth, and therefore benefits from continuous linear time-invariant (LTI) methods.\\nAfter ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To\\ndisambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6.\\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The\\nperformance differences shrink dramatically; this reinforces the hypothesis that layers closer to theraw audio signal should\\nbe LTI, but once they are “tokenized” and compressed by the outer layers, the inner layers no longer need to be LTI. In this')]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/retrievers/#retrievers\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6},\n",
    ")\n",
    "\n",
    "retriever.batch(\n",
    "    [\n",
    "        \"What is the main idea of the document?\",\n",
    "        \"What is the mamba block?\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后面还有东西才对，我们要把vector store 拿到的东西放到prompt里面 然后给到大模型啊\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "query = \"你的问题，比如：What is the main idea of the document?\"\n",
    "results = vector_store.similarity_search(query, k=4)  # k 取你想要的数量\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"你是一个专业的文档问答助手。请根据以下 context 回答用户问题。\\n\\nContext:\\n{context}\"\n",
    "user_prompt = \"Question: {question}\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(system_prompt.format(context=context)),\n",
    "    HumanMessage(user_prompt.format(question=query)),\n",
    "]\n",
    "\n",
    "# TODO: 我记得langchain处理这种rag有专门的chain来着，明天再看看吧\n",
    "def rag_ask(query, vector_store, model, k=4):\n",
    "    results = vector_store.similarity_search(query, k=k)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    system_prompt = \"你是一个专业的文档问答助手。请根据以下 context 回答用户问题。\\n\\nContext:\\n{context}\"\n",
    "    user_prompt = \"Question: {question}\"\n",
    "    messages = [\n",
    "        SystemMessage(system_prompt.format(context=context)),\n",
    "        HumanMessage(user_prompt.format(question=query)),\n",
    "    ]\n",
    "    return model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The main idea of the document revolves around **selective state space models (SSMs)** and their advantages over traditional models like Transformers and linear time-invariant (LTI) SSMs. Key points include:\\n\\n1. **Selectivity in SSMs**:  \\n   - The document highlights how introducing selectivity (e.g., through the parameter Δ) allows SSMs to dynamically focus on or ignore inputs, improving performance with minimal parameter overhead.  \\n   - A small Δ retains the state and ignores the current input, while a large Δ resets the state and focuses on the input (similar to RNN gates).  \\n\\n2. **Performance Improvements**:  \\n   - Increasing the state size (𝑁) in selective SSMs leads to significant perplexity improvements (over 1.0) with only a 1% increase in parameters.  \\n   - Selective SSMs outperform recurrent baselines (e.g., RWKV, RetNet) on long sequences (e.g., 8k context length) due to better efficiency.  \\n\\n3. **Interpretation of Parameters**:  \\n   - **Δ**: Governs input selection and state retention.  \\n   - **𝑨, 𝑩, 𝑪**: Selectivity in Δ is sufficient for performance gains, though making 𝑨 selective could also help.  \\n   - **Boundary Resetting**: Selective SSMs can reset states at sequence boundaries (e.g., document or episode boundaries), preventing information leakage.  \\n\\n4. **Comparison to Other Models**:  \\n   - Selective SSMs address limitations of LTI SSMs (e.g., handling varying sequence contexts) and compete with Transformers, especially in long-context settings.  \\n\\n5. **Future Directions**:  \\n   - The document suggests further exploration of selective mechanisms (e.g., in 𝑨) and applications in areas like reinforcement learning.  \\n\\nIn summary, the core argument is that **selective SSMs offer a computationally efficient and flexible alternative to Transformers and traditional SSMs, with strong empirical results and interpretable dynamics**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 407, 'prompt_tokens': 667, 'total_tokens': 1074, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 667}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '5de6cc79-4fe2-4eec-a57b-f0adb7111526', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--f4d1cefa-94f0-45b1-bb61-d418e8e951c0-0' usage_metadata={'input_tokens': 667, 'output_tokens': 407, 'total_tokens': 1074, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "answer = rag_ask(\"What is the main idea of the document?\", vector_store, model)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the document revolves around **selective state space models (SSMs)** and their advantages over traditional models like Transformers and linear time-invariant (LTI) SSMs. Key points include:\\n\\n1. **Selectivity in SSMs**:  \\n   - The document highlights how introducing selectivity (e.g., through the parameter Δ) allows SSMs to dynamically focus on or ignore inputs, improving performance with minimal parameter overhead.  \\n   - A small Δ retains the state and ignores the current input, while a large Δ resets the state and focuses on the input (similar to RNN gates).  \\n\\n2. **Performance Improvements**:  \\n   - Increasing the state size (𝑁) in selective SSMs leads to significant perplexity improvements (over 1.0) with only a 1% increase in parameters.  \\n   - Selective SSMs outperform recurrent baselines (e.g., RWKV, RetNet) on long sequences (e.g., 8k context length) due to better efficiency.  \\n\\n3. **Interpretation of Parameters**:  \\n   - **Δ**: Governs input selection and state retention.  \\n   - **𝑨, 𝑩, 𝑪**: Selectivity in Δ is sufficient for performance gains, though making 𝑨 selective could also help.  \\n   - **Boundary Resetting**: Selective SSMs can reset states at sequence boundaries (e.g., document or episode boundaries), preventing information leakage.  \\n\\n4. **Comparison to Other Models**:  \\n   - Selective SSMs address limitations of LTI SSMs (e.g., handling varying sequence contexts) and compete with Transformers, especially in long-context settings.  \\n\\n5. **Future Directions**:  \\n   - The document suggests further exploration of selective mechanisms (e.g., in 𝑨) and applications in areas like reinforcement learning.  \\n\\nIn summary, the core argument is that **selective SSMs offer a computationally efficient and flexible alternative to Transformers and traditional SSMs, with strong empirical results and interpretable dynamics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The main idea of the document revolves around **selective state space models (SSMs)** and their advantages over traditional models like Transformers and linear time-invariant (LTI) SSMs. Key points include:\\n\\n1. **Selectivity in SSMs**:  \\n   - The document highlights how introducing selectivity (e.g., through the parameter Δ) allows SSMs to dynamically focus on or ignore inputs, improving performance with minimal parameter overhead.  \\n   - A small Δ retains the state and ignores the current input, while a large Δ resets the state and focuses on the input (similar to RNN gates).  \\n\\n2. **Performance Improvements**:  \\n   - Increasing the state size (𝑁) in selective SSMs leads to significant perplexity improvements (over 1.0) with only a 1% increase in parameters.  \\n   - Selective SSMs outperform recurrent baselines (e.g., RWKV, RetNet) on long sequences (e.g., 8k context length) due to better efficiency.  \\n\\n3. **Interpretation of Parameters**:  \\n   - **Δ**: Governs input selection and state retention.  \\n   - **𝑨, 𝑩, 𝑪**: Selectivity in Δ is sufficient for performance gains, though making 𝑨 selective could also help.  \\n   - **Boundary Resetting**: Selective SSMs can reset states at sequence boundaries (e.g., document or episode boundaries), preventing information leakage.  \\n\\n4. **Comparison to Other Models**:  \\n   - Selective SSMs address limitations of LTI SSMs (e.g., handling varying sequence contexts) and compete with Transformers, especially in long-context settings.  \\n\\n5. **Future Directions**:  \\n   - The document suggests further exploration of selective mechanisms (e.g., in 𝑨) and applications in areas like reinforcement learning.  \\n\\nIn summary, the core argument is that **selective SSMs offer a computationally efficient and flexible alternative to Transformers and traditional SSMs, with strong empirical results and interpretable dynamics**.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 407, 'prompt_tokens': 667, 'total_tokens': 1074, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 667}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'id': '5de6cc79-4fe2-4eec-a57b-f0adb7111526', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--f4d1cefa-94f0-45b1-bb61-d418e8e951c0-0' usage_metadata={'input_tokens': 667, 'output_tokens': 407, 'total_tokens': 1074, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "answer = rag_ask(\"What is the main idea of the document?\", vector_store, model)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
