# 2025/6/25
# zhangzhong
# æ—¢å±•ç¤ºagentçš„å†³ç­–è¿‡ç¨‹ï¼Œåˆå¯ä»¥stream token - è§£å†³æ–¹æ¡ˆ

from dotenv import load_dotenv

load_dotenv()

from langchain_deepseek import ChatDeepSeek
from langchain_tavily import TavilySearch
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
import threading
import time

# åˆå§‹åŒ–
model = ChatDeepSeek(model="deepseek-chat", temperature=0)
search = TavilySearch(max_results=2)
tools = [search]
memory = MemorySaver()
agent_executor = create_react_agent(model, tools, checkpointer=memory)


def stream_with_decisions_and_tokens(input_message):
    """
    åŒæ—¶å±•ç¤ºagentå†³ç­–è¿‡ç¨‹å’Œæµå¼tokenè¾“å‡º
    """
    print("ğŸ¤– å¼€å§‹å¤„ç†...\n")

    # ç”¨äºåŒæ­¥ä¸¤ä¸ªæµ
    decision_complete = threading.Event()
    token_complete = threading.Event()

    def show_decisions():
        """å±•ç¤ºå†³ç­–è¿‡ç¨‹"""
        try:
            for step in agent_executor.stream(
                {"messages": [input_message]}, stream_mode="values"
            ):
                latest_message = step["messages"][-1]

                # æ£€æµ‹å·¥å…·è°ƒç”¨
                if hasattr(latest_message, "tool_calls") and latest_message.tool_calls:
                    print(f"\nğŸ”§ å†³ç­–: è°ƒç”¨å·¥å…· {latest_message.tool_calls[0]['name']}")
                    print(f"   å‚æ•°: {latest_message.tool_calls[0]['args']}")

                # æ£€æµ‹å·¥å…·ç»“æœ
                elif (
                    hasattr(latest_message, "tool_call_id")
                    and latest_message.tool_call_id
                ):
                    print(f"\nğŸ“Š å·¥å…·æ‰§è¡Œå®Œæˆ: {latest_message.tool_name}")

                # æ£€æµ‹AIæ€è€ƒ
                elif hasattr(latest_message, "content") and latest_message.content:
                    if "AI Message" in str(latest_message):
                        print(f"\nğŸ’­ AIæ­£åœ¨æ€è€ƒ...")

        except Exception as e:
            print(f"âŒ å†³ç­–ç›‘æ§é”™è¯¯: {e}")
        finally:
            decision_complete.set()

    def stream_tokens():
        """æµå¼è¾“å‡ºtoken"""
        try:
            print("\nğŸ“ å®æ—¶å“åº”:")
            for step, metadata in agent_executor.stream(
                {"messages": [input_message]}, stream_mode="messages"
            ):
                if metadata["langgraph_node"] == "agent":
                    # æµå¼è¾“å‡ºAIçš„token
                    print(step.text(), end="", flush=True)

                elif metadata["langgraph_node"] == "tool":
                    # å·¥å…·æ‰§è¡Œæç¤º
                    print(f"\nâš¡ æ‰§è¡Œå·¥å…·: {step.tool_name}")

        except Exception as e:
            print(f"âŒ Tokenæµå¼è¾“å‡ºé”™è¯¯: {e}")
        finally:
            token_complete.set()

    # å¯åŠ¨ä¸¤ä¸ªçº¿ç¨‹
    decision_thread = threading.Thread(target=show_decisions)
    token_thread = threading.Thread(target=stream_tokens)

    decision_thread.start()
    token_thread.start()

    # ç­‰å¾…ä¸¤ä¸ªçº¿ç¨‹å®Œæˆ
    decision_thread.join()
    token_thread.join()

    print("\nâœ… å¤„ç†å®Œæˆï¼")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        {"role": "user", "content": "What's the weather in Beijing?"},
        {"role": "user", "content": "Tell me about the latest AI developments"},
    ]

    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i}: {question['content']}")
        print(f"{'='*60}")

        stream_with_decisions_and_tokens(question)
        time.sleep(2)
