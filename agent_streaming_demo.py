# 2025/6/25
# zhangzhong
# æ¼”ç¤ºå¦‚ä½•åŒæ—¶å®ç°agentå†³ç­–è¿‡ç¨‹å±•ç¤ºå’Œtokenæµå¼è¾“å‡º

from dotenv import load_dotenv

load_dotenv()

from langchain_deepseek import ChatDeepSeek
from langchain_tavily import TavilySearch
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
import asyncio
import threading
import time
from queue import Queue

# åˆå§‹åŒ–æ¨¡å‹å’Œå·¥å…·
model = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

search = TavilySearch(max_results=2)
tools = [search]

# åˆ›å»ºagent
memory = MemorySaver()
agent_executor = create_react_agent(model, tools, checkpointer=memory)


class StreamingAgent:
    """ç»“åˆå†³ç­–è¿‡ç¨‹å±•ç¤ºå’Œtokenæµå¼è¾“å‡ºçš„Agentç±»"""

    def __init__(self, agent_executor):
        self.agent_executor = agent_executor
        self.decision_queue = Queue()
        self.token_queue = Queue()

    def stream_with_decision_and_tokens(self, input_message):
        """åŒæ—¶å±•ç¤ºå†³ç­–è¿‡ç¨‹å’Œæµå¼tokenè¾“å‡º"""

        print("ğŸ¤– å¼€å§‹å¤„ç†è¯·æ±‚...\n")

        # å¯åŠ¨å†³ç­–è¿‡ç¨‹ç›‘æ§çº¿ç¨‹
        decision_thread = threading.Thread(
            target=self._monitor_decisions, args=(input_message,)
        )
        decision_thread.start()

        # å¯åŠ¨tokenæµå¼è¾“å‡ºçº¿ç¨‹
        token_thread = threading.Thread(
            target=self._stream_tokens, args=(input_message,)
        )
        token_thread.start()

        # ç­‰å¾…ä¸¤ä¸ªçº¿ç¨‹å®Œæˆ
        decision_thread.join()
        token_thread.join()

        print("\nâœ… å¤„ç†å®Œæˆï¼")

    def _monitor_decisions(self, input_message):
        """ç›‘æ§agentçš„å†³ç­–è¿‡ç¨‹"""
        try:
            for step in self.agent_executor.stream(
                {"messages": [input_message]}, stream_mode="values"
            ):
                latest_message = step["messages"][-1]

                # åˆ†ææ¶ˆæ¯ç±»å‹å¹¶å±•ç¤ºå†³ç­–è¿‡ç¨‹
                if hasattr(latest_message, "tool_calls") and latest_message.tool_calls:
                    print(f"\nğŸ”§ å†³ç­–: è°ƒç”¨å·¥å…· {latest_message.tool_calls[0]['name']}")
                    print(f"   å‚æ•°: {latest_message.tool_calls[0]['args']}")
                    print(f"   è°ƒç”¨ID: {latest_message.tool_calls[0]['id']}")

                elif (
                    hasattr(latest_message, "tool_call_id")
                    and latest_message.tool_call_id
                ):
                    print(f"\nğŸ“Š å·¥å…·æ‰§è¡Œç»“æœ:")
                    print(f"   å·¥å…·: {latest_message.tool_name}")
                    print(f"   ç»“æœ: {str(latest_message.content)[:100]}...")

                elif hasattr(latest_message, "content") and latest_message.content:
                    if "AI Message" in str(latest_message):
                        print(f"\nğŸ’­ AIæ€è€ƒè¿‡ç¨‹: {str(latest_message.content)[:50]}...")

                time.sleep(0.1)  # é¿å…è¾“å‡ºè¿‡å¿«

        except Exception as e:
            print(f"âŒ å†³ç­–ç›‘æ§é”™è¯¯: {e}")

    def _stream_tokens(self, input_message):
        """æµå¼è¾“å‡ºtoken"""
        try:
            print("\nğŸ“ å®æ—¶å“åº”:")
            for step, metadata in self.agent_executor.stream(
                {"messages": [input_message]}, stream_mode="messages"
            ):
                if metadata["langgraph_node"] == "agent":
                    # æµå¼è¾“å‡ºAIçš„token
                    print(step.text(), end="", flush=True)

                elif metadata["langgraph_node"] == "tool":
                    # å·¥å…·æ‰§è¡Œæ—¶çš„æç¤º
                    print(f"\nâš¡ æ­£åœ¨æ‰§è¡Œå·¥å…·: {step.tool_name}")

        except Exception as e:
            print(f"âŒ Tokenæµå¼è¾“å‡ºé”™è¯¯: {e}")


# ä½¿ç”¨ç¤ºä¾‹
def demo_basic_streaming():
    """åŸºç¡€æµå¼è¾“å‡ºæ¼”ç¤º"""
    print("=== åŸºç¡€æµå¼è¾“å‡ºæ¼”ç¤º ===")
    input_message = {"role": "user", "content": "Search for the weather in SF"}

    print("1. åªæ˜¾ç¤ºå†³ç­–è¿‡ç¨‹:")
    for step in agent_executor.stream(
        {"messages": [input_message]}, stream_mode="values"
    ):
        step["messages"][-1].pretty_print()
        break  # åªæ˜¾ç¤ºç¬¬ä¸€æ­¥

    print("\n2. åªæ˜¾ç¤ºtokenæµå¼è¾“å‡º:")
    for step, metadata in agent_executor.stream(
        {"messages": [input_message]}, stream_mode="messages"
    ):
        if metadata["langgraph_node"] == "agent":
            print(step.text(), end="", flush=True)
        break  # åªæ˜¾ç¤ºç¬¬ä¸€æ­¥


def demo_combined_streaming():
    """ç»“åˆå†³ç­–è¿‡ç¨‹å’Œtokenæµå¼è¾“å‡º"""
    print("\n=== ç»“åˆå†³ç­–è¿‡ç¨‹å’Œtokenæµå¼è¾“å‡º ===")

    streaming_agent = StreamingAgent(agent_executor)

    # æµ‹è¯•ä¸åŒçš„é—®é¢˜
    test_questions = [
        {"role": "user", "content": "What's the weather in Beijing?"},
        {"role": "user", "content": "Tell me about the latest AI developments"},
    ]

    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*50}")
        print(f"æµ‹è¯• {i}: {question['content']}")
        print(f"{'='*50}")

        streaming_agent.stream_with_decision_and_tokens(question)
        time.sleep(2)  # é—´éš”


def demo_async_streaming():
    """å¼‚æ­¥æµå¼è¾“å‡ºæ¼”ç¤º"""
    print("\n=== å¼‚æ­¥æµå¼è¾“å‡ºæ¼”ç¤º ===")

    async def async_stream():
        input_message = {
            "role": "user",
            "content": "Search for Python programming tutorials",
        }

        async for step in agent_executor.astream(
            {"messages": [input_message]}, stream_mode="messages"
        ):
            if step[1]["langgraph_node"] == "agent":
                print(step[0].text(), end="", flush=True)

    asyncio.run(async_stream())


if __name__ == "__main__":
    print("ğŸš€ LangGraph Agent æµå¼è¾“å‡ºæ¼”ç¤º")
    print("=" * 60)

    # è¿è¡Œæ¼”ç¤º
    demo_basic_streaming()
    demo_combined_streaming()
    demo_async_streaming()
